import plotly.graph_objects as go
import plotly.express as px
import plotly.utils
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
import base64
import io
import time
import asyncio
import concurrent.futures
from matplotlib import pyplot as plt
import seaborn as sns
import openai
from dotenv import load_dotenv
import os
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher
import re
from datetime import datetime
# ì €ì¥ ê´€ë ¨ import ì œê±°ë¨

# Batch ì²˜ë¦¬ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
try:
    from batch_config import (
        OPENAI_CONFIG, BATCH_CONFIG, MONITORING_CONFIG, 
        FALLBACK_CONFIG, MEMORY_CONFIG, ERROR_HANDLING_CONFIG,
        get_optimal_batch_size, should_use_batch_processing
    )
    print("âœ… Batch ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ batch_config.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # ê¸°ë³¸ ì„¤ì • (fallback)
    OPENAI_CONFIG = {
        "model": "gpt-3.5-turbo",
        "temperature": 0.1,
        "max_tokens": 1000,
        "timeout": 30,
    }
    BATCH_CONFIG = {
        "default_batch_size": 10,
        "max_batch_size": 20,
        "min_batch_size": 5,
        "api_call_delay": 0.1,
        "max_retries": 3,
        "retry_delay": 1.0,
    }
    MONITORING_CONFIG = {"enable_logging": True}
    FALLBACK_CONFIG = {"enable_rule_based_fallback": True}
    MEMORY_CONFIG = {"max_concurrent_batches": 3}
    ERROR_HANDLING_CONFIG = {"continue_on_partial_failure": True}
    
    def get_optimal_batch_size(response_count: int) -> int:
        return min(10, response_count)
    
    def should_use_batch_processing(response_count: int) -> bool:
        return response_count >= 5

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¬¸ì¥ ìœ ì‚¬ë„ ëª¨ë¸ ì´ˆê¸°í™”
try:
    sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    print("âœ… Sentence Transformers ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âŒ Sentence Transformers ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    sentence_model = None
    
# ì €ì¥ ê´€ë ¨ í•¨ìˆ˜ë“¤ ì œê±°ë¨

# ì „ì—­ ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤
# ì €ì¥ ê¸°ëŠ¥ì€ ì œê±°ë¨

class LLMTagger:
    """LLMì„ ì‚¬ìš©í•œ ì˜ë¯¸ íƒœê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.model = OPENAI_CONFIG.get("model", "gpt-3.5-turbo")
        self.batch_size = BATCH_CONFIG.get("default_batch_size", 10)
        self.temperature = OPENAI_CONFIG.get("temperature", 0.1)
        self.max_tokens = OPENAI_CONFIG.get("max_tokens", 1000)
        self.timeout = OPENAI_CONFIG.get("timeout", 30)
    
    def tag_response(self, response_text: str) -> Dict[str, str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ íƒœê¹…"""
        try:
            prompt = f"""
ë‹¤ìŒ ë¬¸ì¥ì„ ê°ì • ë°©í–¥, ê°ì • ê°•ë„, í–‰ë™ ì„±í–¥, ê´€ê³„ ì§€í–¥ì„±, ì§€ì› ìš”ì²­ ì—¬ë¶€ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ë¬¸ì¥: "{response_text}"

ê²°ê³¼ í˜•ì‹:
- ê°ì • ë°©í–¥: ê¸ì •/ë¶€ì •/ì¤‘ë¦½
- ê°ì • ê°•ë„: ì•½í•¨/ë³´í†µ/ê°•í•¨
- í–‰ë™ ì„±í–¥: ìˆ˜ë™ì /ëŠ¥ë™ì /íšŒí”¼ì 
- ê´€ê³„ ì§€í–¥ì„±: ìê¸°ì¤‘ì‹¬/íƒ€ì¸ì§€í–¥/ê· í˜•
- ì§€ì› ìš”ì²­ ì—¬ë¶€: ì•”ì‹œì  ìš”ì²­/ëª…ì‹œì  ìš”ì²­/ì—†ìŒ

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "ê°ì • ë°©í–¥": "ì¤‘ë¦½",
    "ê°ì • ê°•ë„": "ë³´í†µ",
    "í–‰ë™ ì„±í–¥": "ìˆ˜ë™ì ",
    "ê´€ê³„ ì§€í–¥ì„±": "ìê¸°ì¤‘ì‹¬",
    "ì§€ì› ìš”ì²­ ì—¬ë¶€": "ì—†ìŒ"
}}
"""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì‘ë‹µì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì¼ê´€ëœ íƒœê¹…ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=200
            )
            
            # JSON ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content
            print(f"ğŸ” LLM ì›ë³¸ ì‘ë‹µ: {content}")
            
            # JSON ë¸”ë¡ ì œê±°
            if content.startswith('```json'):
                content = content[7:-3]  # ```json ì œê±°
            elif content.startswith('```'):
                content = content[3:-3]  # ``` ì œê±°
            
            print(f"ğŸ” JSON íŒŒì‹± ì‹œë„: {content}")
            
            try:
                result = json.loads(content)
                return result
            except json.JSONDecodeError as json_error:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(json_error)}")
                print(f"ğŸ” ë¬¸ì œê°€ ìˆëŠ” JSON: {content}")
                
                # JSON ìˆ˜ì • ì‹œë„
                try:
                    # ë”°ì˜´í‘œ ë¬¸ì œ ìˆ˜ì •
                    content = content.replace('"', '"').replace('"', '"')
                    # ì¤„ë°”ê¿ˆ ì œê±°
                    content = content.replace('\n', '').replace('\r', '')
                    # ê³µë°± ì •ë¦¬
                    content = content.strip()
                    
                    print(f"ğŸ” ìˆ˜ì •ëœ JSON: {content}")
                    result = json.loads(content)
                    return result
                except:
                    print(f"âŒ JSON ìˆ˜ì • ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ íƒœê¹…ìœ¼ë¡œ ëŒ€ì²´")
                    return self._rule_based_tagging(response_text)
            
        except Exception as e:
            print(f"âŒ LLM íƒœê¹… ì‹¤íŒ¨: {str(e)}")
            # LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ íƒœê¹…ìœ¼ë¡œ ëŒ€ì²´
            return self._rule_based_tagging(response_text)
    
    def tag_responses_batch(self, responses: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """ì—¬ëŸ¬ ì‘ë‹µì„ ë³‘ë ¬ Batch ì²˜ë¦¬í•˜ì—¬ ê³ ì† ì˜ë¯¸ íƒœê¹…"""
        try:
            if not responses:
                return []
            
            print(f"ğŸš€ ê³ ì† ë³‘ë ¬ Batch íƒœê¹… ì‹œì‘: {len(responses)}ê°œ ì‘ë‹µ")
            
            # ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° (ë” í° ë°°ì¹˜ë¡œ ì²˜ë¦¬)
            optimal_batch_size = get_optimal_batch_size(len(responses))
            batches = [responses[i:i + optimal_batch_size] for i in range(0, len(responses), optimal_batch_size)]
            
            print(f"ğŸ“¦ ë°°ì¹˜ êµ¬ì„±: {len(batches)}ê°œ ë°°ì¹˜, ë°°ì¹˜ë‹¹ {optimal_batch_size}ê°œ ì‘ë‹µ")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì‚¬ìš©
            max_workers = min(MEMORY_CONFIG.get("max_concurrent_batches", 5), len(batches))
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  ë°°ì¹˜ë¥¼ ë™ì‹œì— ì œì¶œ
                future_to_batch = {
                    executor.submit(self._process_batch, batch, batch_idx): batch_idx 
                    for batch_idx, batch in enumerate(batches)
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                all_results = []
                completed_batches = 0
                
                for future in concurrent.futures.as_completed(future_to_batch):
                    batch_idx = future_to_batch[future]
                    try:
                        batch_results = future.result()
                        all_results.extend(batch_results)
                        completed_batches += 1
                        print(f"âœ… Batch {batch_idx + 1}/{len(batches)} ì™„ë£Œ ({completed_batches}/{len(batches)})")
                    except Exception as e:
                        print(f"âŒ Batch {batch_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                        # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ê·œì¹™ ê¸°ë°˜ íƒœê¹…ìœ¼ë¡œ ëŒ€ì²´
                        batch = batches[batch_idx]
                        fallback_results = [self._rule_based_tagging(resp['text']) for resp in batch]
                        all_results.extend(fallback_results)
                        completed_batches += 1
            
            print(f"ğŸ‰ ê³ ì† ë³‘ë ¬ Batch íƒœê¹… ì™„ë£Œ: {len(all_results)}ê°œ ì‘ë‹µ ì²˜ë¦¬ë¨")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if MEMORY_CONFIG.get("enable_garbage_collection", True):
                import gc
                gc.collect()
                print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            return all_results
            
        except Exception as e:
            print(f"âŒ ë³‘ë ¬ Batch íƒœê¹… ì‹¤íŒ¨: {str(e)}")
            print("ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ë¡œ ëŒ€ì²´...")
            return self._sequential_fallback(responses)
    
    def _process_batch(self, batch: List[Dict[str, str]], batch_idx: int) -> List[Dict[str, str]]:
        """ê°œë³„ ë°°ì¹˜ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
        try:
            print(f"ğŸ“¦ Batch {batch_idx + 1} ì²˜ë¦¬ ì‹œì‘ ({len(batch)}ê°œ ì‘ë‹µ)")
            
            # Batchìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
            batch_prompt = self._create_batch_prompt(batch)
            
            # OpenAI API í˜¸ì¶œ (ìµœì í™”ëœ ì¬ì‹œë„ ë¡œì§)
            max_retries = BATCH_CONFIG.get("max_retries", 2)
            retry_delay = BATCH_CONFIG.get("retry_delay", 0.5)
            
            for retry_attempt in range(max_retries):
                try:
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì‘ë‹µì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ì‘ë‹µì„ ì •í™•í•˜ê³  ì¼ê´€ë˜ê²Œ íƒœê¹…í•´ì£¼ì„¸ìš”."},
                            {"role": "user", "content": batch_prompt}
                        ],
                        temperature=self.temperature,
                        max_tokens=self.max_tokens
                    )
                    break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
                except Exception as e:
                    if retry_attempt < max_retries - 1:
                        print(f"âš ï¸ Batch {batch_idx + 1} API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {retry_attempt + 1}/{max_retries}): {str(e)}")
                        time.sleep(retry_delay)
                    else:
                        print(f"âŒ Batch {batch_idx + 1} ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                        raise e
            
            # Batch ê²°ê³¼ íŒŒì‹±
            content = response.choices[0].message.content
            batch_results = self._parse_batch_response(content, len(batch))
            
            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì™„
            for i, result in enumerate(batch_results):
                if not result or not isinstance(result, dict):
                    print(f"âš ï¸ Batch {batch_idx + 1}, ì‘ë‹µ {i + 1} íŒŒì‹± ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ íƒœê¹…ìœ¼ë¡œ ëŒ€ì²´")
                    batch_results[i] = self._rule_based_tagging(batch[i]['text'])
            
            print(f"âœ… Batch {batch_idx + 1} ì²˜ë¦¬ ì™„ë£Œ")
            return batch_results
            
        except Exception as e:
            print(f"âŒ Batch {batch_idx + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê·œì¹™ ê¸°ë°˜ íƒœê¹…ìœ¼ë¡œ ëŒ€ì²´
            return [self._rule_based_tagging(resp['text']) for resp in batch]
    
    def _sequential_fallback(self, responses: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """ìˆœì°¨ ì²˜ë¦¬ í´ë°± (ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ)"""
        print("ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ë¡œ ëŒ€ì²´ ì¤‘...")
        results = []
        for i, resp in enumerate(responses):
            try:
                result = self.tag_response(resp['text'])
                results.append(result)
                if (i + 1) % 10 == 0:
                    print(f"ğŸ“ ìˆœì°¨ ì²˜ë¦¬ ì§„í–‰ë¥ : {i + 1}/{len(responses)}")
            except Exception as e:
                print(f"âš ï¸ ì‘ë‹µ {i + 1} ì²˜ë¦¬ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ íƒœê¹…ìœ¼ë¡œ ëŒ€ì²´: {str(e)}")
                result = self._rule_based_tagging(resp['text'])
                results.append(result)
        return results
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return {
            "batch_size": BATCH_CONFIG.get("default_batch_size", 25),
            "max_concurrent": MEMORY_CONFIG.get("max_concurrent_batches", 5),
            "api_delay": BATCH_CONFIG.get("api_call_delay", 0.05),
            "retry_count": BATCH_CONFIG.get("max_retries", 2)
        }
    

    
    def _create_batch_prompt(self, responses: List[Dict[str, str]]) -> str:
        """Batch ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = """ë‹¤ìŒ ì‘ë‹µë“¤ì„ ê°ì • ë°©í–¥, ê°ì • ê°•ë„, í–‰ë™ ì„±í–¥, ê´€ê³„ ì§€í–¥ì„±, ì§€ì› ìš”ì²­ ì—¬ë¶€ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ê° ì‘ë‹µì— ëŒ€í•´ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

ì‘ë‹µë“¤:
"""
        
        for i, resp in enumerate(responses):
            prompt += f"{i + 1}. {resp['text']}\n"
        
        prompt += """
ê²°ê³¼ í˜•ì‹:
[
    {
        "ê°ì • ë°©í–¥": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½",
        "ê°ì • ê°•ë„": "ì•½í•¨/ë³´í†µ/ê°•í•¨",
        "í–‰ë™ ì„±í–¥": "ìˆ˜ë™ì /ëŠ¥ë™ì /íšŒí”¼ì ",
        "ê´€ê³„ ì§€í–¥ì„±": "ìê¸°ì¤‘ì‹¬/íƒ€ì¸ì§€í–¥/ê· í˜•",
        "ì§€ì› ìš”ì²­ ì—¬ë¶€": "ì•”ì‹œì  ìš”ì²­/ëª…ì‹œì  ìš”ì²­/ì—†ìŒ"
    },
    ...
]
"""
        return prompt
    
    def _parse_batch_response(self, content: str, expected_count: int) -> List[Dict[str, str]]:
        """Batch ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ë¸”ë¡ ì œê±°
            if content.startswith('```json'):
                content = content[7:-3]
            elif content.startswith('```'):
                content = content[3:-3]
            
            # JSON íŒŒì‹±
            result = json.loads(content)
            
            if isinstance(result, list) and len(result) == expected_count:
                return result
            else:
                print(f"âš ï¸ Batch ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: ì˜ˆìƒ {expected_count}ê°œ, ì‹¤ì œ {len(result) if isinstance(result, list) else 'not list'}")
                return []
                
        except json.JSONDecodeError as e:
            print(f"âŒ Batch ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ” ë¬¸ì œê°€ ìˆëŠ” ë‚´ìš©: {content}")
            return []
        except Exception as e:
            print(f"âŒ Batch ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _rule_based_tagging(self, response_text: str) -> Dict[str, str]:
        """ê·œì¹™ ê¸°ë°˜ íƒœê¹… (LLM ì‹¤íŒ¨ ì‹œ ëŒ€ì²´)"""
        result = {}
        
        # ê°ì • ë°©í–¥
        positive_words = ['í–‰ë³µ', 'ê¸°ì¨', 'ë§Œì¡±', 'í¬ë§', 'ê°ì‚¬', 'ì¢‹ë‹¤', 'í›Œë¥­í•˜ë‹¤']
        negative_words = ['ìŠ¬í””', 'í™”ë‚¨', 'ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì§œì¦', 'ë‚˜ì˜ë‹¤', 'í˜ë“¤ë‹¤']
        
        positive_count = sum(response_text.count(word) for word in positive_words)
        negative_count = sum(response_text.count(word) for word in negative_words)
        
        if positive_count > negative_count:
            result["ê°ì • ë°©í–¥"] = "ê¸ì •"
        elif negative_count > positive_count:
            result["ê°ì • ë°©í–¥"] = "ë¶€ì •"
        else:
            result["ê°ì • ë°©í–¥"] = "ì¤‘ë¦½"
        
        # ê°ì • ê°•ë„
        emotion_words = positive_words + negative_words
        emotion_count = sum(response_text.count(word) for word in emotion_words)
        
        if emotion_count >= 3:
            result["ê°ì • ê°•ë„"] = "ê°•í•¨"
        elif emotion_count >= 1:
            result["ê°ì • ê°•ë„"] = "ë³´í†µ"
        else:
            result["ê°ì • ê°•ë„"] = "ì•½í•¨"
        
        # í–‰ë™ ì„±í–¥
        active_words = ['í•  ê²ƒì´ë‹¤', 'í•˜ë ¤ê³ ', 'ë…¸ë ¥', 'ì˜ì§€', 'ëª©í‘œ', 'ê³„íš']
        passive_words = ['ë  ê²ƒ ê°™ë‹¤', 'ì•„ë§ˆë„', 'ì–´ì©Œë©´', 'ê·¸ëƒ¥', 'ê¸°ë‹¤ë¦°ë‹¤']
        avoidant_words = ['í”¼í•˜ë‹¤', 'íšŒí”¼', 'ë„ë§', 'ìˆ¨ê¸°ë‹¤']
        
        active_count = sum(response_text.count(word) for word in active_words)
        passive_count = sum(response_text.count(word) for word in passive_words)
        avoidant_count = sum(response_text.count(word) for word in avoidant_words)
        
        if active_count > passive_count and active_count > avoidant_count:
            result["í–‰ë™ ì„±í–¥"] = "ëŠ¥ë™ì "
        elif avoidant_count > active_count and avoidant_count > passive_count:
            result["í–‰ë™ ì„±í–¥"] = "íšŒí”¼ì "
        else:
            result["í–‰ë™ ì„±í–¥"] = "ìˆ˜ë™ì "
        
        # ê´€ê³„ ì§€í–¥ì„±
        self_words = ['ë‚˜', 'ë‚´', 'ì €', 'ì œ', 'ìì‹ ', 'ê°œì¸']
        other_words = ['ìš°ë¦¬', 'í•¨ê»˜', 'ì‚¬íšŒ', 'ê³µë™ì²´', 'í˜‘ë ¥', 'ì†Œí†µ']
        
        self_count = sum(response_text.count(word) for word in self_words)
        other_count = sum(response_text.count(word) for word in other_words)
        
        if self_count > other_count * 2:
            result["ê´€ê³„ ì§€í–¥ì„±"] = "ìê¸°ì¤‘ì‹¬"
        elif other_count > self_count * 2:
            result["ê´€ê³„ ì§€í–¥ì„±"] = "íƒ€ì¸ì§€í–¥"
        else:
            result["ê´€ê³„ ì§€í–¥ì„±"] = "ê· í˜•"
        
        # ì§€ì› ìš”ì²­ ì—¬ë¶€
        explicit_request = ['ë„ì™€ì£¼ì„¸ìš”', 'ì¡°ì–¸í•´ì£¼ì„¸ìš”', 'ê°€ë¥´ì³ì£¼ì„¸ìš”', 'ìƒë‹´ë°›ê³  ì‹¶ì–´ìš”']
        implicit_request = ['ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”', 'ë°©ë²•ì„ ëª¨ë¥´ê² ì–´ìš”', 'í˜ë“¤ì–´ìš”']
        
        explicit_count = sum(response_text.count(word) for word in explicit_request)
        implicit_count = sum(response_text.count(word) for word in implicit_request)
        
        if explicit_count > 0:
            result["ì§€ì› ìš”ì²­ ì—¬ë¶€"] = "ëª…ì‹œì  ìš”ì²­"
        elif implicit_count > 0:
            result["ì§€ì› ìš”ì²­ ì—¬ë¶€"] = "ì•”ì‹œì  ìš”ì²­"
        else:
            result["ì§€ì› ìš”ì²­ ì—¬ë¶€"] = "ì—†ìŒ"
        
        return result

class VisualizationGenerator:
    def __init__(self):
        self.colors = ['#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0', '#9966FF', '#FF9F40']
        self.llm_tagger = LLMTagger()  # LLM íƒœê±° ì´ˆê¸°í™”
    
    def generate_radar_chart(self, data: Dict[str, List[float]], labels: List[str], title: str = "í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ íƒœê·¸ ë¹„êµ") -> Dict[str, Any]:
        """Plotlyë¥¼ ì‚¬ìš©í•œ ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
        fig = go.Figure()
        
        for i, (persona, values) in enumerate(data.items()):
            color = self.colors[i % len(self.colors)]
            # rgba í˜•ì‹ìœ¼ë¡œ íˆ¬ëª…ë„ ì¶”ê°€ (0.4 = 40%)
            rgba_color = f"rgba({int(color[1:3], 16)}, {int(color[3:5], 16)}, {int(color[5:7], 16)}, 0.4)"
            
            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=labels,
                fill='toself',
                name=f'í˜ë¥´ì†Œë‚˜ {persona}',
                line=dict(color=color, width=3),
                marker=dict(color=color, size=10, symbol='circle'),
                fillcolor=rgba_color,
                opacity=0.6
            ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True, 
                    range=[0, 5],
                    tickmode='array',
                    tickvals=[0, 1, 2, 3, 4, 5],
                    ticktext=['0', '1', '2', '3', '4', '5'],
                    tickfont=dict(size=12),
                    tickcolor='#666'
                ),
                angularaxis=dict(
                    tickfont=dict(size=14, color='#333'),
                    tickcolor='#333'
                ),
                bgcolor='#f8f9fa'
            ),
            title=dict(
                text=title,
                font=dict(size=18),
                x=0.5,
                y=0.95
            ),
            showlegend=True,
            legend=dict(
                x=0.5,
                y=0.85,
                xanchor='center',
                yanchor='top',
                orientation='h',
                bgcolor='rgba(255,255,255,0.8)',
                bordercolor='#ccc',
                borderwidth=1
            ),
            plot_bgcolor='#ffffff',
            paper_bgcolor='#ffffff',
            margin=dict(t=100, b=50, l=50, r=50),
            height=500
        )
        
        return json.loads(fig.to_json())
    
    def generate_heatmap(self, data: Dict[str, List[float]], labels: List[str], title: str = "í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ ë¶„ì„ íˆíŠ¸ë§µ") -> str:
        """Seabornì„ ì‚¬ìš©í•œ íˆíŠ¸ë§µ ìƒì„± ë° base64 ì¸ì½”ë”©"""
        try:
            # matplotlib ë°±ì—”ë“œ ì„¤ì • (Windows í˜¸í™˜ì„±)
            import matplotlib
            matplotlib.use('Agg')
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(data, index=[f"í˜ë¥´ì†Œë‚˜ {k}" for k in data.keys()])
            df.columns = labels
            
            # íˆíŠ¸ë§µ ìƒì„±
            plt.figure(figsize=(10, 6))
            sns.heatmap(
                df, 
                annot=True, 
                cmap="RdYlGn_r", 
                linewidths=0.5,
                fmt='.2f',
                cbar_kws={'label': 'ê°’ (0-5)'}
            )
            plt.title(title, fontsize=16, pad=20)
            plt.xlabel('ì˜ë¯¸ ì¶•', fontsize=12)
            plt.ylabel('í˜ë¥´ì†Œë‚˜', fontsize=12)
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            plt.tight_layout()
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            plt.close()
            buffer.close()
            
            return image_base64
            
        except Exception as e:
            print(f"âŒ íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê°„ë‹¨í•œ HTML í…Œì´ë¸”ë¡œ ëŒ€ì²´
            return self._generate_fallback_heatmap(data, labels, title)
    
    def _generate_fallback_heatmap(self, data: Dict[str, List[float]], labels: List[str], title: str) -> str:
        """íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ HTML í…Œì´ë¸” ìƒì„±"""
        try:
            html_content = f"""
            <div style="text-align: center; padding: 20px;">
                <h4 style="margin-bottom: 20px; color: #495057;">{title}</h4>
                <table style="width: 100%; border-collapse: collapse; margin: 0 auto;">
                    <thead>
                        <tr style="background-color: #f8f9fa;">
                            <th style="border: 1px solid #dee2e6; padding: 12px; text-align: center;">í˜ë¥´ì†Œë‚˜</th>
            """
            
            for label in labels:
                html_content += f'<th style="border: 1px solid #dee2e6; padding: 12px; text-align: center;">{label}</th>'
            
            html_content += "</tr></thead><tbody>"
            
            for persona, values in data.items():
                html_content += f'<tr><td style="border: 1px solid #dee2e6; padding: 12px; font-weight: bold;">{persona}</td>'
                for value in values:
                    # ìƒ‰ìƒ ê°•ë„ì— ë”°ë¥¸ ë°°ê²½ìƒ‰ ì„¤ì •
                    intensity = min(5, max(0, value))
                    normalized = intensity / 5
                    red = int(255 * (1 - normalized))
                    green = int(255 * normalized)
                    blue = 100
                    html_content += f'<td style="border: 1px solid #dee2e6; padding: 12px; text-align: center; background-color: rgb({red},{green},{blue}); color: white; font-weight: bold;">{intensity:.1f}</td>'
                html_content += "</tr>"
            
            html_content += "</tbody></table></div>"
            
            # HTMLì„ base64ë¡œ ì¸ì½”ë”© (ê°„ë‹¨í•œ ëŒ€ì•ˆ)
            return base64.b64encode(html_content.encode()).decode()
            
        except Exception as e:
            print(f"âŒ ëŒ€ì²´ íˆíŠ¸ë§µ ìƒì„±ë„ ì‹¤íŒ¨: {str(e)}")
            return "ì˜¤ë¥˜: íˆíŠ¸ë§µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def generate_plotly_heatmap(self, data: Dict[str, List[float]], labels: List[str], title: str = "í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ ë¶„ì„ íˆíŠ¸ë§µ") -> Dict[str, Any]:
        """Plotlyë¥¼ ì‚¬ìš©í•œ ì¸í„°ë™í‹°ë¸Œ íˆíŠ¸ë§µ ìƒì„±"""
        try:
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(data, index=[f"í˜ë¥´ì†Œë‚˜ {k}" for k in data.keys()])
            df.columns = labels
            
            # Plotly íˆíŠ¸ë§µ ìƒì„±
            fig = px.imshow(
                df, 
                color_continuous_scale="RdBu_r",  # ë¹¨ê°•-íŒŒë‘ (ì—­ë°©í–¥)
                text_auto=True,
                aspect="auto"
            )
            
            # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
            fig.update_layout(
                title=dict(
                    text=title,
                    font=dict(size=18),
                    x=0.5,
                    y=0.95
                ),
                xaxis=dict(
                    title='ì˜ë¯¸ ì¶•',
                    titlefont=dict(size=14),
                    tickangle=45,
                    tickfont=dict(size=12)
                ),
                yaxis=dict(
                    title='í˜ë¥´ì†Œë‚˜',
                    titlefont=dict(size=14),
                    tickfont=dict(size=12)
                ),
                coloraxis=dict(
                    colorbar=dict(
                        title="ê°’ (0-5)",
                        titleside="right",
                        thickness=20,
                        len=0.8
                    )
                ),
                plot_bgcolor='#ffffff',
                paper_bgcolor='#ffffff',
                margin=dict(t=100, b=50, l=50, r=50),
                height=500
            )
            
            # í…ìŠ¤íŠ¸ í‘œì‹œ ê°œì„ 
            fig.update_traces(
                texttemplate="%{z:.2f}",
                textfont=dict(size=12, color="white"),
                hovertemplate='<b>%{y}</b><br><b>%{x}</b><br>ê°’: %{z:.2f}<extra></extra>'
            )
            
            return json.loads(fig.to_json())
            
        except Exception as e:
            print(f"âŒ Plotly íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ì¡´ Seaborn íˆíŠ¸ë§µìœ¼ë¡œ ëŒ€ì²´
            return {"error": f"Plotly íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {str(e)}", "fallback": "seaborn"}
    
    def generate_sorting_chart(self, data: Dict[str, float], title: str = "ì˜ë¯¸ ì¶• ê¸°ì¤€ ì •ë ¬") -> Dict[str, Any]:
        """Plotlyë¥¼ ì‚¬ìš©í•œ ì •ë ¬ ì°¨íŠ¸ ìƒì„±"""
        # ê°’ì— ë”°ë¼ ì •ë ¬
        sorted_data = sorted(data.items(), key=lambda x: x[1], reverse=True)
        personas = [item[0] for item in sorted_data]
        values = [item[1] for item in sorted_data]
        
        # ìƒ‰ìƒ ìƒì„± (ìƒìœ„ 3ê°œëŠ” íŠ¹ë³„í•œ ìƒ‰ìƒ)
        colors = []
        for i in range(len(personas)):
            if i < 3:
                colors.append('#FFD700')  # ê¸ˆìƒ‰
            else:
                colors.append(self.colors[i % len(self.colors)])
        
        fig = go.Figure(data=[
            go.Bar(
                x=personas,
                y=values,
                marker_color=colors,
                text=[f'{v:.2f}' for v in values],
                textposition='auto',
                hovertemplate='<b>%{x}</b><br>ê°’: %{y:.2f}<extra></extra>'
            )
        ])
        
        fig.update_layout(
            title=dict(
                text=title,
                font=dict(size=18),
                x=0.5,
                y=0.95
            ),
            xaxis=dict(
                title='í˜ë¥´ì†Œë‚˜',
                titlefont=dict(size=14),
                tickangle=45
            ),
            yaxis=dict(
                title='ê°’',
                titlefont=dict(size=14),
                range=[0, max(values) * 1.1]
            ),
            plot_bgcolor='#ffffff',
            paper_bgcolor='#ffffff',
            height=400,
            showlegend=False
        )
        
        return json.loads(fig.to_json())
    
    def generate_side_by_side_diff(self, response_a: str, response_b: str, persona_a: str, persona_b: str, question: str) -> Dict[str, Any]:
        """Side-by-Side Diff ì‹œê°í™” ìƒì„±"""
        try:
            # 1. ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_score = self._calculate_similarity(response_a, response_b)
            
            # 2. ì˜ë¯¸ ì°¨ì´ ë¶„ì„
            analysis_a = self.analyze_response_dimensions(response_a)
            analysis_b = self.analyze_response_dimensions(response_b)
            
            # 3. ì°¨ì´ì  í•˜ì´ë¼ì´íŠ¸
            diff_highlights = self._highlight_differences(response_a, response_b)
            
            # 4. ì˜ë¯¸ ì¶•ë³„ ì°¨ì´ ê³„ì‚°
            dimension_diffs = self._calculate_dimension_differences(analysis_a, analysis_b)
            
            # 5. ì‹œê°í™” ë°ì´í„° êµ¬ì„±
            diff_data = {
                'question': question,
                'persona_a': persona_a,
                'persona_b': persona_b,
                'response_a': response_a,
                'response_b': response_b,
                'similarity_score': similarity_score,
                'analysis_a': analysis_a,
                'analysis_b': analysis_b,
                'diff_highlights': diff_highlights,
                'dimension_differences': dimension_diffs,
                'summary': self._generate_diff_summary(analysis_a, analysis_b, similarity_score)
            }
            
            return diff_data
            
        except Exception as e:
            print(f"âŒ Side-by-Side Diff ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return self._generate_fallback_diff(response_a, response_b, persona_a, persona_b, question)
    
    def _calculate_similarity(self, text_a: str, text_b: str) -> float:
        """ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if sentence_model:
                # Sentence Transformers ì‚¬ìš©
                embeddings_a = sentence_model.encode([text_a])
                embeddings_b = sentence_model.encode([text_b])
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(embeddings_a[0], embeddings_b[0]) / (
                    np.linalg.norm(embeddings_a[0]) * np.linalg.norm(embeddings_b[0])
                )
                return float(similarity)
            else:
                # difflib ì‚¬ìš© (ëŒ€ì²´ ë°©ë²•)
                return SequenceMatcher(None, text_a, text_b).ratio()
                
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def _highlight_differences(self, text_a: str, text_b: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì°¨ì´ì  í•˜ì´ë¼ì´íŠ¸"""
        try:
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            words_a = re.findall(r'\w+', text_a.lower())
            words_b = re.findall(r'\w+', text_b.lower())
            
            # ê³µí†µ ë‹¨ì–´ì™€ ê³ ìœ  ë‹¨ì–´ ì°¾ê¸°
            common_words = set(words_a) & set(words_b)
            unique_a = set(words_a) - set(words_b)
            unique_b = set(words_b) - set(words_a)
            
            # ê°ì • ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ë¥˜
            emotion_keywords = {
                'positive': ['í–‰ë³µ', 'ê¸°ì¨', 'ë§Œì¡±', 'í¬ë§', 'ì¢‹ë‹¤', 'í›Œë¥­í•˜ë‹¤'],
                'negative': ['ìŠ¬í””', 'í™”ë‚¨', 'ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì§œì¦', 'ë‚˜ì˜ë‹¤', 'í˜ë“¤ë‹¤'],
                'neutral': ['ë³´í†µ', 'ì¼ë°˜ì ', 'í‰ë²”', 'ì¤‘ê°„']
            }
            
            # ê° ì‘ë‹µì˜ ê°ì • í‚¤ì›Œë“œ ë¶„ì„
            emotion_a = self._analyze_emotion_keywords(text_a, emotion_keywords)
            emotion_b = self._analyze_emotion_keywords(text_b, emotion_keywords)
            
            return {
                'common_words': list(common_words)[:10],  # ìƒìœ„ 10ê°œ
                'unique_a': list(unique_a)[:10],
                'unique_b': list(unique_b)[:10],
                'emotion_a': emotion_a,
                'emotion_b': emotion_b,
                'word_count_a': len(words_a),
                'word_count_b': len(words_b)
            }
            
        except Exception as e:
            print(f"âŒ ì°¨ì´ì  í•˜ì´ë¼ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _analyze_emotion_keywords(self, text: str, emotion_keywords: Dict[str, List[str]]) -> Dict[str, int]:
        """ê°ì • í‚¤ì›Œë“œ ë¶„ì„"""
        result = {}
        for emotion_type, keywords in emotion_keywords.items():
            count = sum(text.count(keyword) for keyword in keywords)
            result[emotion_type] = count
        return result
    
    def _calculate_dimension_differences(self, analysis_a: Dict[str, float], analysis_b: Dict[str, float]) -> Dict[str, Dict[str, Any]]:
        """ì˜ë¯¸ ì¶•ë³„ ì°¨ì´ ê³„ì‚°"""
        dimensions = ['emotional_intensity', 'valence', 'expression_type', 'agency', 'extroversion', 'solution_offered']
        differences = {}
        
        for dim in dimensions:
            value_a = analysis_a.get(dim, 0.0)
            value_b = analysis_b.get(dim, 0.0)
            
            diff = abs(value_a - value_b)
            diff_percentage = (diff / max(value_a, value_b)) * 100 if max(value_a, value_b) > 0 else 0
            
            # ì°¨ì´ ì •ë„ ë¶„ë¥˜
            if diff_percentage > 50:
                diff_level = "í° ì°¨ì´"
            elif diff_percentage > 25:
                diff_level = "ì¤‘ê°„ ì°¨ì´"
            else:
                diff_level = "ì‘ì€ ì°¨ì´"
            
            differences[dim] = {
                'value_a': round(value_a, 2),
                'value_b': round(value_b, 2),
                'difference': round(diff, 2),
                'difference_percentage': round(diff_percentage, 1),
                'difference_level': diff_level,
                'trend': "ë†’ìŒ" if value_a > value_b else "ë‚®ìŒ" if value_a < value_b else "ë™ì¼"
            }
        
        return differences
    
    def _generate_diff_summary(self, analysis_a: Dict[str, float], analysis_b: Dict[str, float], similarity: float) -> str:
        """ì°¨ì´ì  ìš”ì•½ ìƒì„±"""
        try:
            # ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” ì°¨ì› ì°¾ê¸°
            max_diff_dim = None
            max_diff = 0
            
            for dim in ['emotional_intensity', 'valence', 'expression_type', 'agency', 'extroversion', 'solution_offered']:
                diff = abs(analysis_a.get(dim, 0) - analysis_b.get(dim, 0))
                if diff > max_diff:
                    max_diff = diff
                    max_diff_dim = dim
            
            # ìœ ì‚¬ë„ì— ë”°ë¥¸ ì „ì²´ì ì¸ í‰ê°€
            if similarity > 0.8:
                overall_assessment = "ë§¤ìš° ìœ ì‚¬í•œ ì‘ë‹µ"
            elif similarity > 0.6:
                overall_assessment = "ë¹„ìŠ·í•œ ì‘ë‹µ"
            elif similarity > 0.4:
                overall_assessment = "ì¤‘ê°„ ì •ë„ì˜ ì°¨ì´"
            else:
                overall_assessment = "ë§¤ìš° ë‹¤ë¥¸ ì‘ë‹µ"
            
            # ìš”ì•½ ë¬¸ì¥ ìƒì„±
            if max_diff_dim:
                dim_name = self.get_dimension_display_name(max_diff_dim)
                summary = f"{overall_assessment}ì…ë‹ˆë‹¤. ê°€ì¥ í° ì°¨ì´ëŠ” '{dim_name}'ì—ì„œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤."
            else:
                summary = f"{overall_assessment}ì…ë‹ˆë‹¤."
            
            return summary
            
        except Exception as e:
            print(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return "ì‘ë‹µ ë¹„êµ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
    
    def _generate_fallback_diff(self, response_a: str, response_b: str, persona_a: str, persona_b: str, question: str) -> Dict[str, Any]:
        """Fallback diff ë°ì´í„° ìƒì„±"""
        return {
            'question': question,
            'persona_a': persona_a,
            'persona_b': persona_b,
            'response_a': response_a,
            'response_b': response_b,
            'similarity_score': 0.5,
            'analysis_a': {},
            'analysis_b': {},
            'diff_highlights': {},
            'dimension_differences': {},
            'summary': 'ê¸°ë³¸ ë¹„êµ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'
        }
    
    def analyze_response_dimensions(self, response_text: str) -> Dict[str, float]:
        """LLM íƒœê¹…ê³¼ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ì„ ê²°í•©í•˜ì—¬ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„"""
        if not response_text:
            return {}
        
        # LLM íƒœê¹… ì‹œë„
        try:
            llm_tags = self.llm_tagger.tag_response(response_text)
            print(f"ğŸ” LLM íƒœê¹… ê²°ê³¼: {llm_tags}")
            
            # LLM íƒœê¹… ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ ë³€í™˜
            analysis = {}
            
            # ê°ì • ë°©í–¥: ê¸ì •(5), ì¤‘ë¦½(3), ë¶€ì •(1)
            valence_map = {"ê¸ì •": 5, "ì¤‘ë¦½": 3, "ë¶€ì •": 1}
            analysis['valence'] = valence_map.get(llm_tags.get("ê°ì • ë°©í–¥", "ì¤‘ë¦½"), 3)
            
            # ê°ì • ê°•ë„: ê°•í•¨(5), ë³´í†µ(3), ì•½í•¨(1)
            intensity_map = {"ê°•í•¨": 5, "ë³´í†µ": 3, "ì•½í•¨": 1}
            analysis['emotional_intensity'] = intensity_map.get(llm_tags.get("ê°ì • ê°•ë„", "ë³´í†µ"), 3)
            
            # í–‰ë™ ì„±í–¥: ëŠ¥ë™ì (5), ìˆ˜ë™ì (3), íšŒí”¼ì (1)
            agency_map = {"ëŠ¥ë™ì ": 5, "ìˆ˜ë™ì ": 3, "íšŒí”¼ì ": 1}
            analysis['agency'] = agency_map.get(llm_tags.get("í–‰ë™ ì„±í–¥", "ìˆ˜ë™ì "), 3)
            
            # ê´€ê³„ ì§€í–¥ì„±: íƒ€ì¸ì§€í–¥(5), ê· í˜•(3), ìê¸°ì¤‘ì‹¬(1)
            extroversion_map = {"íƒ€ì¸ì§€í–¥": 5, "ê· í˜•": 3, "ìê¸°ì¤‘ì‹¬": 1}
            analysis['extroversion'] = extroversion_map.get(llm_tags.get("ê´€ê³„ ì§€í–¥ì„±", "ê· í˜•"), 3)
            
            # ì§€ì› ìš”ì²­ ì—¬ë¶€: ëª…ì‹œì (5), ì•”ì‹œì (3), ì—†ìŒ(1)
            solution_map = {"ëª…ì‹œì  ìš”ì²­": 5, "ì•”ì‹œì  ìš”ì²­": 3, "ì—†ìŒ": 1}
            analysis['solution_offered'] = solution_map.get(llm_tags.get("ì§€ì› ìš”ì²­ ì—¬ë¶€", "ì—†ìŒ"), 1)
            
            # í‘œí˜„ ìŠ¤íƒ€ì¼ (ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜)
            comfort_words = ['ê´œì°®ì•„', 'í˜ë‚´', 'ì˜ë  ê±°ì•¼', 'ê±±ì •ë§ˆ', 'ìœ„ë¡œ', 'ì•ˆì‹¬', 'í¬ë§', 'ê¸°ëŒ€']
            info_words = ['ì •ë³´', 'ì‚¬ì‹¤', 'ë°ì´í„°', 'í†µê³„', 'ì—°êµ¬', 'ë¶„ì„', 'ê²°ê³¼', 'ì¦ê±°']
            command_words = ['í•´ì•¼', 'í•˜ì§€ë§ˆ', 'í•„ìš”í•´', 'ì¤‘ìš”í•´', 'ë‹¹ì—°í•´', 'ë¬´ì¡°ê±´']
            
            comfort_score = sum(response_text.count(word) for word in comfort_words) * 0.6
            info_score = sum(response_text.count(word) for word in info_words) * 0.7
            command_score = sum(response_text.count(word) for word in command_words) * 0.5
            
            style_scores = [comfort_score, info_score, command_score]
            max_style = max(style_scores)
            if max_style > 0:
                if max_style == comfort_score:
                    analysis['expression_type'] = 1  # ìœ„ë¡œí˜•
                elif max_style == info_score:
                    analysis['expression_type'] = 3  # ì •ë³´í˜•
                else:
                    analysis['expression_type'] = 5  # ëª…ë ¹í˜•
            else:
                analysis['expression_type'] = 3  # ì¤‘ë¦½
            
            # ì‘ë‹µ ê¸¸ì´ ë° ë³µì¡ë„ (ë³´ì¡° ì§€í‘œ)
            analysis['response_length'] = min(5, max(1, len(response_text) / 100))
            
            sentences = [s.strip() for s in response_text.split('.') if s.strip()]
            if sentences:
                avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
                analysis['complexity'] = min(5, max(1, avg_sentence_length / 30))
            else:
                analysis['complexity'] = 3
            
            return analysis
            
        except Exception as e:
            print(f"âŒ LLM íƒœê¹… ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´: {str(e)}")
            # LLM ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©
            return self._legacy_analyze_response_dimensions(response_text)
    
    def analyze_responses_batch(self, responses: List[str]) -> List[Dict[str, float]]:
        """ì—¬ëŸ¬ ì‘ë‹µì„ Batchë¡œ ì²˜ë¦¬í•˜ì—¬ ì˜ë¯¸ ë¶„ì„"""
        if not responses:
            return []
        
        start_time = time.time()
        print(f"ğŸš€ Batch ë¶„ì„ ì‹œì‘: {len(responses)}ê°œ ì‘ë‹µ")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        use_batch = should_use_batch_processing(len(responses))
        if use_batch:
            print(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™” (ì‘ë‹µ ìˆ˜: {len(responses)})")
        else:
            print(f"ğŸ“Š ê°œë³„ ì²˜ë¦¬ ì‚¬ìš© (ì‘ë‹µ ìˆ˜: {len(responses)})")
        
        # ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        optimal_batch_size = get_optimal_batch_size(len(responses))
        batches = [responses[i:i + optimal_batch_size] for i in range(0, len(responses), optimal_batch_size)]
        all_analyses = []
        
        for batch_idx, batch in enumerate(batches):
            print(f"ğŸ“¦ Batch {batch_idx + 1}/{len(batches)} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
            
            # Batch íƒœê¹… ìˆ˜í–‰
            batch_responses = [{'text': resp} for resp in batch]
            batch_tags = self.llm_tagger.tag_responses_batch(batch_responses)
            
            # ê° ì‘ë‹µì— ëŒ€í•´ ë¶„ì„ ìˆ˜í–‰
            for i, (response_text, tags) in enumerate(zip(batch, batch_tags)):
                try:
                    # íƒœê·¸ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ ë³€í™˜
                    analysis = {}
                    
                    # ê°ì • ë°©í–¥: ê¸ì •(5), ì¤‘ë¦½(3), ë¶€ì •(1)
                    valence_map = {"ê¸ì •": 5, "ì¤‘ë¦½": 3, "ë¶€ì •": 1}
                    analysis['valence'] = valence_map.get(tags.get("ê°ì • ë°©í–¥", "ì¤‘ë¦½"), 3)
                    
                    # ê°ì • ê°•ë„: ê°•í•¨(5), ë³´í†µ(3), ì•½í•¨(1)
                    intensity_map = {"ê°•í•¨": 5, "ë³´í†µ": 3, "ì•½í•¨": 1}
                    analysis['emotional_intensity'] = intensity_map.get(tags.get("ê°ì • ê°•ë„", "ë³´í†µ"), 3)
                    
                    # í–‰ë™ ì„±í–¥: ëŠ¥ë™ì (5), ìˆ˜ë™ì (3), íšŒí”¼ì (1)
                    agency_map = {"ëŠ¥ë™ì ": 5, "ìˆ˜ë™ì ": 3, "íšŒí”¼ì ": 1}
                    analysis['agency'] = agency_map.get(tags.get("í–‰ë™ ì„±í–¥", "ìˆ˜ë™ì "), 3)
                    
                    # ê´€ê³„ ì§€í–¥ì„±: íƒ€ì¸ì§€í–¥(5), ê· í˜•(3), ìê¸°ì¤‘ì‹¬(1)
                    extroversion_map = {"íƒ€ì¸ì§€í–¥": 5, "ê· í˜•": 3, "ìê¸°ì¤‘ì‹¬": 1}
                    analysis['extroversion'] = extroversion_map.get(tags.get("ê´€ê³„ ì§€í–¥ì„±", "ê· í˜•"), 3)
                    
                    # ì§€ì› ìš”ì²­ ì—¬ë¶€: ëª…ì‹œì (5), ì•”ì‹œì (3), ì—†ìŒ(1)
                    solution_map = {"ëª…ì‹œì  ìš”ì²­": 5, "ì•”ì‹œì  ìš”ì²­": 3, "ì—†ìŒ": 1}
                    analysis['solution_offered'] = solution_map.get(tags.get("ì§€ì› ìš”ì²­ ì—¬ë¶€", "ì—†ìŒ"), 1)
                    
                    # í‘œí˜„ ìŠ¤íƒ€ì¼ (ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜)
                    comfort_words = ['ê´œì°®ì•„', 'í˜ë‚´', 'ì˜ë  ê±°ì•¼', 'ê±±ì •ë§ˆ', 'ìœ„ë¡œ', 'ì•ˆì‹¬', 'í¬ë§', 'ê¸°ëŒ€']
                    info_words = ['ì •ë³´', 'ì‚¬ì‹¤', 'ë°ì´í„°', 'í†µê³„', 'ì—°êµ¬', 'ë¶„ì„', 'ê²°ê³¼', 'ì¦ê±°']
                    command_words = ['í•´ì•¼', 'í•˜ì§€ë§ˆ', 'í•„ìš”í•´', 'ì¤‘ìš”í•´', 'ë‹¹ì—°í•´', 'ë¬´ì¡°ê±´']
                    
                    comfort_score = sum(response_text.count(word) for word in comfort_words) * 0.6
                    info_score = sum(response_text.count(word) for word in info_words) * 0.7
                    command_score = sum(response_text.count(word) for word in command_words) * 0.5
                    
                    style_scores = [comfort_score, info_score, command_score]
                    max_style = max(style_scores)
                    if max_style > 0:
                        if max_style == comfort_score:
                            analysis['expression_type'] = 1  # ìœ„ë¡œí˜•
                        elif max_style == info_score:
                            analysis['expression_type'] = 3  # ì •ë³´í˜•
                        else:
                            analysis['expression_type'] = 1  # ëª…ë ¹í˜•
                    else:
                        analysis['expression_type'] = 3  # ì¤‘ë¦½
                    
                    # ì‘ë‹µ ê¸¸ì´ ë° ë³µì¡ë„ (ë³´ì¡° ì§€í‘œ)
                    analysis['response_length'] = min(5, max(1, len(response_text) / 100))
                    
                    sentences = [s.strip() for s in response_text.split('.') if s.strip()]
                    if sentences:
                        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
                        analysis['complexity'] = min(5, max(1, avg_sentence_length / 30))
                    else:
                        analysis['complexity'] = 3
                    
                    all_analyses.append(analysis)
                    
                except Exception as e:
                    print(f"âš ï¸ Batch {batch_idx + 1}, ì‘ë‹µ {i + 1} ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    # ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
                    fallback_analysis = self._legacy_analyze_response_dimensions(response_text)
                    all_analyses.append(fallback_analysis)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        if MONITORING_CONFIG.get("log_performance_metrics", True):
            print(f"ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
            print(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"   - ì‘ë‹µë‹¹ í‰ê·  ì‹œê°„: {processing_time/len(responses):.3f}ì´ˆ")
            print(f"   - ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±: {'í™œì„±í™”' if use_batch else 'ë¹„í™œì„±í™”'}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„¤ì •ì— ë”°ë¼)
        if MEMORY_CONFIG.get("enable_garbage_collection", True):
            import gc
            gc.collect()
            print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        print(f"âœ… Batch ë¶„ì„ ì™„ë£Œ: {len(all_analyses)}ê°œ ì‘ë‹µ ì²˜ë¦¬ë¨")
        return all_analyses
    
    def _legacy_analyze_response_dimensions(self, response_text: str) -> Dict[str, float]:
        """ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (LLM ì‹¤íŒ¨ ì‹œ ëŒ€ì²´)"""
        if not response_text:
            return {}
        
        analysis = {}
        
        # 1. ê°ì • ê°•ë„ (Emotional Intensity) - ì‘ë‹µì´ ì–¼ë§ˆë‚˜ ê°ì •ì /ëƒ‰ì •í•œê°€
        emotion_words = {
            'high': ['í–‰ë³µ', 'ê¸°ì¨', 'ë§Œì¡±', 'í¬ë§', 'ê°ì‚¬', 'ì‚¬ë‘', 'ì¦ê±°ì›€', 'ì›ƒìŒ', 
                    'ìŠ¬í””', 'í™”ë‚¨', 'ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì§œì¦', 'ì ˆë§', 'ê±±ì •', 'ìŠ¤íŠ¸ë ˆìŠ¤'],
            'medium': ['í‰ì˜¨', 'ì°¨ë¶„', 'ë³´í†µ', 'ì¼ë°˜ì ', 'í‰ë²”'],
            'low': ['ë¬´ê´€ì‹¬', 'ì¤‘ë¦½', 'ê°ê´€ì ', 'ì‚¬ì‹¤ì ', 'ë¶„ì„ì ']
        }
        
        high_emotion = sum(response_text.count(word) for word in emotion_words['high']) * 0.4
        medium_emotion = sum(response_text.count(word) for word in emotion_words['medium']) * 0.2
        low_emotion = sum(response_text.count(word) for word in emotion_words['low']) * 0.3
        
        analysis['emotional_intensity'] = min(5, max(1, high_emotion + medium_emotion + low_emotion))
        
        # 2. ì •ì„œ ë°©í–¥ (Valence) - ì‘ë‹µì´ ê¸ì •/ì¤‘ë¦½/ë¶€ì • ì¤‘ ì–´ë””ì¯¤ì¸ê°€
        positive_words = ['í–‰ë³µ', 'ê¸°ì¨', 'ë§Œì¡±', 'í¬ë§', 'ê°ì‚¬', 'ì‚¬ë‘', 'ì¦ê±°ì›€', 'ì›ƒìŒ', 'ì¢‹ë‹¤', 'í›Œë¥­í•˜ë‹¤']
        negative_words = ['ìŠ¬í””', 'í™”ë‚¨', 'ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì§œì¦', 'ì ˆë§', 'ê±±ì •', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ë‚˜ì˜ë‹¤', 'í˜ë“¤ë‹¤']
        
        positive_score = sum(response_text.count(word) for word in positive_words) * 0.5
        negative_score = sum(response_text.count(word) for word in negative_words) * 0.5
        
        # 1(ë¶€ì •) ~ 3(ì¤‘ë¦½) ~ 5(ê¸ì •) ë²”ìœ„ë¡œ ì •ê·œí™”
        valence_score = 3 + (positive_score - negative_score) * 0.5
        analysis['valence'] = min(5, max(1, valence_score))
        
        # 3. í‘œí˜„ ìŠ¤íƒ€ì¼ (Expression Type) - ì‘ë‹µì´ ìœ„ë¡œ ì¤‘ì‹¬ì¸ì§€ ì •ë³´ ì¤‘ì‹¬ì¸ì§€
        comfort_words = ['ê´œì°®ì•„', 'í˜ë‚´', 'ì˜ë  ê±°ì•¼', 'ê±±ì •ë§ˆ', 'ìœ„ë¡œ', 'ì•ˆì‹¬', 'í¬ë§', 'ê¸°ëŒ€']
        info_words = ['ì •ë³´', 'ì‚¬ì‹¤', 'ë°ì´í„°', 'í†µê³„', 'ì—°êµ¬', 'ë¶„ì„', 'ê²°ê³¼', 'ì¦ê±°']
        command_words = ['í•´ì•¼', 'í•˜ì§€ë§ˆ', 'í•„ìš”í•´', 'ì¤‘ìš”í•´', 'ë‹¹ì—°í•´', 'ë¬´ì¡°ê±´']
        
        comfort_score = sum(response_text.count(word) for word in comfort_words) * 0.6
        info_score = sum(response_text.count(word) for word in info_words) * 0.7
        command_score = sum(response_text.count(word) for word in command_words) * 0.5
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ìŠ¤íƒ€ì¼ì„ ì„ íƒ
        style_scores = [comfort_score, info_score, command_score]
        max_style = max(style_scores)
        if max_style > 0:
            if max_style == comfort_score:
                analysis['expression_type'] = 1  # ìœ„ë¡œí˜•
            elif max_style == info_score:
                analysis['expression_type'] = 3  # ì •ë³´í˜•
            else:
                analysis['expression_type'] = 5  # ëª…ë ¹í˜•
        else:
            analysis['expression_type'] = 3  # ì¤‘ë¦½
        
        # 4. ìê¸° ì£¼ë„ì„± (Agency) - ì‘ë‹µìê°€ ëŠ¥ë™ì /ìˆ˜ë™ì /íƒ€ì¸ ì˜ì¡´ì ì¸ê°€
        active_words = ['í•  ê²ƒì´ë‹¤', 'í•˜ë ¤ê³ ', 'ë…¸ë ¥', 'ì˜ì§€', 'ëª©í‘œ', 'ê³„íš', 'ê²°ì‹¬', 'ì§ì ‘', 'ìŠ¤ìŠ¤ë¡œ']
        passive_words = ['ë  ê²ƒ ê°™ë‹¤', 'ì•„ë§ˆë„', 'ì–´ì©Œë©´', 'ê·¸ëƒ¥', 'ê·¸ëŒ€ë¡œ', 'ê¸°ë‹¤ë¦°ë‹¤']
        dependent_words = ['ë„ì›€', 'ì˜ì¡´', 'ìƒë‹´', 'ì¡°ì–¸', 'ê°€ë¥´ì¹¨', 'ì§€ì‹œ', 'ëª…ë ¹']
        
        active_score = sum(response_text.count(word) for word in active_words) * 0.6
        passive_score = sum(response_text.count(word) for word in passive_words) * 0.4
        dependent_score = sum(response_text.count(word) for word in dependent_words) * 0.5
        
        # 1(ìˆ˜ë™) ~ 3(ì¤‘ê°„) ~ 5(ëŠ¥ë™) ë²”ìœ„ë¡œ ì •ê·œí™”
        agency_score = 3 + (active_score - passive_score - dependent_score) * 0.3
        analysis['agency'] = min(5, max(1, agency_score))
        
        # 5. ì™¸í–¥ì„± (Extroversion) - ì‘ë‹µì´ ëŒ€ì™¸ ì§€í–¥ì ì¸ê°€ ë‚´í–¥ì ì¸ê°€
        extrovert_words = ['ìš°ë¦¬', 'í•¨ê»˜', 'ì‚¬íšŒ', 'ê³µë™ì²´', 'í˜‘ë ¥', 'ì†Œí†µ', 'ì´í•´', 'ê³µê°', 'ì¹œêµ¬', 'ê°€ì¡±']
        introvert_words = ['ë‚˜', 'ê°œì¸', 'í˜¼ì', 'ìì‹ ', 'ë‚´ë©´', 'ì‚¬ìƒ‰', 'ê³ ë¯¼', 'ìƒê°']
        
        extrovert_score = sum(response_text.count(word) for word in extrovert_words) * 0.5
        introvert_score = sum(response_text.count(word) for word in introvert_words) * 0.5
        
        # 1(ë‚´í–¥ì ) ~ 3(ì¤‘ê°„) ~ 5(ì™¸í–¥ì ) ë²”ìœ„ë¡œ ì •ê·œí™”
        extroversion_score = 3 + (extrovert_score - introvert_score) * 0.4
        analysis['extroversion'] = min(5, max(1, extroversion_score))
        
        # 6. í•´ê²° ì „ëµ ì œì‹œ ì—¬ë¶€ (Solution Offered) - êµ¬ì²´ì ì¸ ì œì•ˆì´ ìˆëŠ”ê°€
        solution_words = ['í•´ê²°', 'ë°©ë²•', 'ëŒ€ì•ˆ', 'ì‹œë„', 'ì‹¤í–‰', 'ì ì‘', 'ê·¹ë³µ', 'ì œì•ˆ', 'ê¶Œì¥', 'ê³„íš']
        indirect_words = ['ìƒê°í•´ë³´ì', 'ê³ ë¯¼í•´ë³´ì', 'ì‹œê°„ì´ í•„ìš”í•˜ë‹¤', 'ì°¨ê·¼ì°¨ê·¼']
        
        direct_solution = sum(response_text.count(word) for word in solution_words) * 0.8
        indirect_solution = sum(response_text.count(word) for word in indirect_words) * 0.4
        
        if direct_solution > 0:
            analysis['solution_offered'] = 5  # ëª…í™•
        elif indirect_solution > 0:
            analysis['solution_offered'] = 3  # ê°„ì ‘ì 
        else:
            analysis['solution_offered'] = 5  # ì—†ìŒ
        
        # ì‘ë‹µ ê¸¸ì´ ë° ë³µì¡ë„ (ë³´ì¡° ì§€í‘œ)
        analysis['response_length'] = min(5, max(1, len(response_text) / 100))
        
        sentences = [s.strip() for s in response_text.split('.') if s.strip()]
        if sentences:
            avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
            analysis['complexity'] = min(5, max(1, avg_sentence_length / 30))
        else:
            analysis['complexity'] = 3
        
        return analysis
    
    def get_dimension_display_name(self, dimension: str) -> str:
        """ì˜ë¯¸ ì¶•ì˜ í‘œì‹œ ì´ë¦„ì„ ë°˜í™˜"""
        display_names = {
            'emotional_intensity': 'ê°ì • ê°•ë„',
            'valence': 'ì •ì„œ ë°©í–¥',
            'expression_type': 'í‘œí˜„ ìŠ¤íƒ€ì¼',
            'agency': 'ìê¸° ì£¼ë„ì„±',
            'extroversion': 'ì™¸í–¥ì„±',
            'solution_offered': 'ì§€ì› ìš”ì²­',
            'response_length': 'ì‘ë‹µ ê¸¸ì´',
            'complexity': 'ì‘ë‹µ ë³µì¡ë„'
        }
        
        if dimension.startswith('temperament_'):
            key = dimension.replace('temperament_', '')
            return f'ê¸°ì§ˆ: {key}'
        elif dimension.startswith('character_'):
            key = dimension.replace('character_', '')
            return f'ì„±ê²©: {key}'
        
        return display_names.get(dimension, dimension)

from fastapi import APIRouter, Body, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (import í›„ì— ìƒì„±)

router = APIRouter(prefix="/visualization", tags=["visualization"])

# ìƒˆë¡œìš´ êµ¬ì¡°í™”ëœ JSON ìŠ¤í‚¤ë§ˆ ëª¨ë¸ë“¤
class PersonaTag(BaseModel):
    """í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ íƒœê·¸"""
    ê°ì •_ë°©í–¥: str = Field(..., description="ê¸ì •/ë¶€ì •/ì¤‘ë¦½")
    ê°ì •_ê°•ë„: str = Field(..., description="ì•½í•¨/ë³´í†µ/ê°•í•¨")
    í–‰ë™_ì„±í–¥: str = Field(..., description="ìˆ˜ë™ì /ëŠ¥ë™ì /íšŒí”¼ì ")
    ê´€ê³„_ì§€í–¥ì„±: str = Field(..., description="ìê¸°ì¤‘ì‹¬/íƒ€ì¸ì§€í–¥/ê· í˜•")
    ì§€ì›_ìš”ì²­_ì—¬ë¶€: str = Field(..., description="ì•”ì‹œì  ìš”ì²­/ëª…ì‹œì  ìš”ì²­/ì—†ìŒ")
    í‘œí˜„_ìŠ¤íƒ€ì¼: str = Field(..., description="ìœ„ë¡œí˜•/ì •ë³´í˜•/ëª…ë ¹í˜•")

class PersonaResponse(BaseModel):
    """í˜ë¥´ì†Œë‚˜ë³„ ì‘ë‹µ ë°ì´í„°"""
    persona: str = Field(..., description="í˜ë¥´ì†Œë‚˜ ì´ë¦„")
    text: str = Field(..., description="ì‘ë‹µ í…ìŠ¤íŠ¸")
    embedding: Optional[List[float]] = Field(None, description="ë¬¸ì¥ ì„ë² ë”© ë²¡í„°")
    tags: PersonaTag = Field(..., description="ì˜ë¯¸ íƒœê·¸")
    analysis_scores: Dict[str, float] = Field(..., description="ì˜ë¯¸ ì¶•ë³„ ìˆ˜ì¹˜ ì ìˆ˜ (0-5)")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="ì¶”ê°€ ë©”íƒ€ë°ì´í„°")

class SimilarityScore(BaseModel):
    """í˜ë¥´ì†Œë‚˜ ê°„ ìœ ì‚¬ë„ ì ìˆ˜"""
    persona_a: str = Field(..., description="í˜ë¥´ì†Œë‚˜ A")
    persona_b: str = Field(..., description="í˜ë¥´ì†Œë‚˜ B")
    score: float = Field(..., description="ìœ ì‚¬ë„ ì ìˆ˜ (0-1)")
    similarity_type: str = Field(default="cosine", description="ìœ ì‚¬ë„ ê³„ì‚° ë°©ì‹")

class AnalysisResult(BaseModel):
    """ì „ì²´ ë¶„ì„ ê²°ê³¼"""
    question_id: str = Field(..., description="ì§ˆë¬¸ ID")
    question_text: str = Field(..., description="ì§ˆë¬¸ í…ìŠ¤íŠ¸")
    persona_responses: List[PersonaResponse] = Field(..., description="í˜ë¥´ì†Œë‚˜ë³„ ì‘ë‹µ ëª©ë¡")
    similarities: List[SimilarityScore] = Field(..., description="í˜ë¥´ì†Œë‚˜ ê°„ ìœ ì‚¬ë„ ëª©ë¡")
    overall_statistics: Dict[str, Any] = Field(..., description="ì „ì²´ í†µê³„ ì •ë³´")
    model_info: Dict[str, str] = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì •ë³´")

class VisualizationRequest(BaseModel):
    experiment_data: dict
    question_index: int
    dimension: str
    analysis_type: str = "all"  # "radar", "heatmap", "sorting", "all"
    heatmap_type: str = "plotly"  # "plotly", "seaborn"

class DiffRequest(BaseModel):
    experiment_data: dict
    question_index: int
    persona_a: str
    persona_b: str

class StructuredAnalysisRequest(BaseModel):
    """êµ¬ì¡°í™”ëœ ë¶„ì„ ìš”ì²­"""
    experiment_data: dict
    question_index: int
    question_text: str = Field(default="", description="í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ ì§ˆë¬¸ í…ìŠ¤íŠ¸")
    include_embeddings: bool = Field(default=False, description="ì„ë² ë”© í¬í•¨ ì—¬ë¶€")
    include_similarities: bool = Field(default=True, description="ìœ ì‚¬ë„ ê³„ì‚° í¬í•¨ ì—¬ë¶€")
    analysis_depth: str = Field(default="standard", description="ë¶„ì„ ê¹Šì´: basic/standard/detailed")

@router.post("/generate")
async def generate_visualizations(request: VisualizationRequest):
    """ì‹¤í—˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œê°í™” ìƒì„±"""
    try:
        experiment_data = request.experiment_data
        question_index = request.question_index
        dimension = request.dimension
        analysis_type = request.analysis_type
        
        # ì‹¤í—˜ëª… ì¶”ì¶œ
        experiment_name = experiment_data.get('experiment_name', f'experiment_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
        
        print(f"ğŸ”„ ìƒˆë¡œìš´ ë¶„ì„ ìƒì„± ì‹œì‘: {experiment_name}, Q{question_index}")
        
        # ì‘ë‹µ ë°ì´í„° ìˆ˜ì§‘
        responses_data = {}
        dimension_values = {}
        
        print(f"ğŸ” ë””ë²„ê¹…: experiment_data êµ¬ì¡° = {type(experiment_data)}")
        print(f"ğŸ” ë””ë²„ê¹…: experiment_data.keys() = {list(experiment_data.keys()) if isinstance(experiment_data, dict) else 'Not a dict'}")
        
        # ë” ìì„¸í•œ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
        if isinstance(experiment_data, dict):
            print(f"ğŸ” ë””ë²„ê¹…: experiment_data ì „ì²´ êµ¬ì¡°:")
            for key, value in experiment_data.items():
                if isinstance(value, list):
                    print(f"  - {key}: ë¦¬ìŠ¤íŠ¸ (ê¸¸ì´: {len(value)})")
                    if len(value) > 0 and isinstance(value[0], dict):
                        print(f"    ì²« ë²ˆì§¸ í•­ëª© í‚¤: {list(value[0].keys())}")
                elif isinstance(value, dict):
                    print(f"  - {key}: ë”•ì…”ë„ˆë¦¬ (í‚¤: {list(value.keys())})")
                else:
                    print(f"  - {key}: {type(value)} = {value}")
        
        # ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡° ì‹œë„
        prompts_data = []
        
        # ë°©ë²• 1: history[0].answers êµ¬ì¡° (ì‹¤ì œ ë°ì´í„° êµ¬ì¡° - ìš°ì„  ì²˜ë¦¬)
        if 'history' in experiment_data and isinstance(experiment_data['history'], list) and len(experiment_data['history']) > 0:
            print(f"ğŸ” ë””ë²„ê¹…: history í‚¤ ë°œê²¬, ê¸¸ì´ = {len(experiment_data['history'])}")
            print(f"ğŸ” ë””ë²„ê¹…: history[0] í‚¤ë“¤ = {list(experiment_data['history'][0].keys())}")
            if 'answers' in experiment_data['history'][0]:
                prompts_data = experiment_data['history'][0]['answers']
                print(f"ğŸ” ë””ë²„ê¹…: history[0].answersì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
                if len(prompts_data) > 0:
                    print(f"ğŸ” ë””ë²„ê¹…: ì²« ë²ˆì§¸ answers í•­ëª© í‚¤ë“¤ = {list(prompts_data[0].keys())}")
            elif 'prompts' in experiment_data['history'][0]:
                prompts_data = experiment_data['history'][0]['prompts']
                print(f"ğŸ” ë””ë²„ê¹…: history[0].promptsì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
            else:
                print(f"ğŸ” ë””ë²„ê¹…: history[0]ì— answersë‚˜ prompts í‚¤ê°€ ì—†ìŒ")
        # ë°©ë²• 2: ì§ì ‘ prompts í‚¤
        elif 'prompts' in experiment_data:
            prompts_data = experiment_data['prompts']
            print(f"ğŸ” ë””ë²„ê¹…: ì§ì ‘ prompts í‚¤ì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
        
        # ë°©ë²• 4: experiment_dataê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
        elif isinstance(experiment_data, list):
            prompts_data = experiment_data
            print(f"ğŸ” ë””ë²„ê¹…: experiment_dataê°€ ë¦¬ìŠ¤íŠ¸ì„, ê¸¸ì´ = {len(prompts_data)}")
        
        if prompts_data:
            print(f"ğŸ” ë””ë²„ê¹…: prompts_data ê°œìˆ˜ = {len(prompts_data)}")
            
            # Batch ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ë‹µ ë°ì´í„° ìˆ˜ì§‘
            batch_responses = []
            personality_mapping = {}
            
            for i, prompt in enumerate(prompts_data):
                print(f"ğŸ” ë””ë²„ê¹…: prompt[{i}] êµ¬ì¡° = {type(prompt)}")
                print(f"ğŸ” ë””ë²„ê¹…: prompt[{i}].keys() = {list(prompt.keys()) if isinstance(prompt, dict) else 'Not a dict'}")
                
                personality = prompt.get('personality', f'Unknown_{i}')
                print(f"ğŸ” ë””ë²„ê¹…: personality = {personality}")
                
                # prompt ì „ì²´ ë‚´ìš© ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                print(f"ğŸ” ë””ë²„ê¹…: prompt[{i}] ì „ì²´ ë‚´ìš© = {prompt}")
                
                # ë‹¤ì–‘í•œ ì‘ë‹µ ë°ì´í„° êµ¬ì¡° ì‹œë„
                response_text = ""
                
                # ë°©ë²• 1: answer í•„ë“œ ì°¾ê¸° (í˜„ì¬ ë°ì´í„° êµ¬ì¡°ì— ë§ìŒ)
                if 'answer' in prompt:
                    response_text = prompt['answer']
                    print(f"ğŸ” ë””ë²„ê¹…: answer í•„ë“œ ì°¾ìŒ = {response_text[:50]}...")
                    print(f"ğŸ” ë””ë²„ê¹…: answer í•„ë“œ íƒ€ì… = {type(response_text)}")
                    print(f"ğŸ” ë””ë²„ê¹…: answer í•„ë“œ ê¸¸ì´ = {len(str(response_text))}")
                    print(f"ğŸ” ë””ë²„ê¹…: answer í•„ë“œ ë‚´ìš© = '{response_text}'")
                    print(f"ğŸ” ë””ë²„ê¹…: answer í•„ë“œê°€ ë¹„ì–´ìˆë‚˜? = {not response_text or not response_text.strip()}")
                
                # ë°©ë²• 2: ê¸°íƒ€ ì‘ë‹µ í•„ë“œë“¤ (ëŒ€ì•ˆ)
                elif 'response' in prompt:
                    response_text = prompt['response']
                    print(f"ğŸ” ë””ë²„ê¹…: response í•„ë“œ ì°¾ìŒ = {response_text[:50]}...")
                elif 'text' in prompt:
                    response_text = prompt['text']
                    print(f"ğŸ” ë””ë²„ê¹…: text í•„ë“œ ì°¾ìŒ = {response_text[:50]}...")
                
                print(f"ğŸ” ë””ë²„ê¹…: ìµœì¢… response_text ê¸¸ì´ = {len(response_text)}")
                
                if response_text and response_text.strip():
                    # Batch ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ë‹µ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                    batch_responses.append(response_text)
                    personality_mapping[len(batch_responses) - 1] = personality
                else:
                    print(f"ğŸ” ë””ë²„ê¹…: {personality} ì‘ë‹µ í…ìŠ¤íŠ¸ ì—†ìŒ")
            
            # Batch ë¶„ì„ ìˆ˜í–‰
            if batch_responses:
                print(f"ğŸš€ Batch ë¶„ì„ ì‹œì‘: {len(batch_responses)}ê°œ ì‘ë‹µ")
                batch_analyses = viz_generator.analyze_responses_batch(batch_responses)
                
                # ê²°ê³¼ë¥¼ personalityë³„ë¡œ ë§¤í•‘
                for i, analysis in enumerate(batch_analyses):
                    if i in personality_mapping:
                        personality = personality_mapping[i]
                        responses_data[personality] = analysis
                        
                        # ì„ íƒëœ ì°¨ì›ì˜ ê°’ ì¶”ì¶œ
                        if dimension in analysis:
                            dimension_values[personality] = analysis[dimension]
                        else:
                            dimension_values[personality] = np.random.uniform(1, 4)  # ê¸°ë³¸ê°’
                        
                        print(f"ğŸ” ë””ë²„ê¹…: {personality} ì‘ë‹µ ë¶„ì„ ì™„ë£Œ, ì°¨ì›ê°’ = {dimension_values[personality]}")
            else:
                print(f"ğŸ” ë””ë²„ê¹…: ë¶„ì„í•  ì‘ë‹µì´ ì—†ìŒ")
        else:
            print(f"ğŸ” ë””ë²„ê¹…: prompts ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        print(f"ğŸ” ë””ë²„ê¹…: ìµœì¢… responses_data ê°œìˆ˜ = {len(responses_data)}")
        print(f"ğŸ” ë””ë²„ê¹…: ìµœì¢… dimension_values ê°œìˆ˜ = {len(dimension_values)}")
        
        if not responses_data:
            if not prompts_data:
                print("âŒ ì‘ë‹µ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: answers ë°°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ í™”ë©´ì— ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ëœ¨ì§€ ì•Šë„ë¡ í•¨
                return {
                    "success": True,
                    "data": {
                        "sorting_chart": None,
                        "sorting_list": None
                    },
                    "message": "ë¶„ì„í•  ì‘ë‹µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
                }
            else:
                print("âŒ ì‘ë‹µ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ í™”ë©´ì— ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ëœ¨ì§€ ì•Šë„ë¡ í•¨
                return {
                    "success": True,
                    "data": {
                        "sorting_chart": None,
                        "sorting_list": None
                    },
                    "message": "ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }
        
        result = {}
        
        # ë ˆì´ë” ì°¨íŠ¸ ìƒì„±
        if analysis_type in ["radar", "all"]:
            try:
                print(f"ğŸ” ë ˆì´ë” ì°¨íŠ¸ ë””ë²„ê¹…: responses_data í‚¤ ê°œìˆ˜ = {len(responses_data)}")
                print(f"ğŸ” ë ˆì´ë” ì°¨íŠ¸ ë””ë²„ê¹…: ì²« ë²ˆì§¸ ì‘ë‹µ ë°ì´í„° í‚¤ = {list(responses_data.keys())[:3]}")
                
                # ëª¨ë“  ì°¨ì›ì— ëŒ€í•œ ë°ì´í„° ì¤€ë¹„
                all_dimensions = ['emotional_intensity', 'valence', 'expression_type', 
                                'agency', 'extroversion', 'solution_offered', 'response_length', 'complexity']
                
                radar_data = {}
                for persona, analysis in responses_data.items():
                    values = []
                    for dim in all_dimensions:
                        values.append(analysis.get(dim, 0.0))  # ê¸°ë³¸ê°’ 0.0
                    radar_data[persona] = values
                    print(f"ğŸ” ë ˆì´ë” ì°¨íŠ¸ ë””ë²„ê¹…: {persona} = {values}")
                
                print(f"ğŸ” ë ˆì´ë” ì°¨íŠ¸ ë””ë²„ê¹…: radar_data í‚¤ ê°œìˆ˜ = {len(radar_data)}")
                print(f"ğŸ” ë ˆì´ë” ì°¨íŠ¸ ë””ë²„ê¹…: ì²« ë²ˆì§¸ ê°’ ê¸¸ì´ = {len(list(radar_data.values())[0])}")
                
                result['radar_chart'] = viz_generator.generate_radar_chart(
                    radar_data, 
                    all_dimensions,
                    f"ì§ˆë¬¸ {question_index + 1}: í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ íƒœê·¸ ë¹„êµ"
                )
            except Exception as e:
                print(f"âŒ ë ˆì´ë” ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë ˆì´ë” ì°¨íŠ¸ ë°ì´í„° ìƒì„±
                fallback_radar = {}
                for persona in responses_data.keys():
                    fallback_radar[persona] = [2.0, 2.5, 3.0, 2.5, 2.0, 2.5, 3.0, 2.5]
                
                result['radar_chart'] = viz_generator.generate_radar_chart(
                    fallback_radar,
                    ['ê°ì • ê°•ë„', 'ì •ì„œ ë°©í–¥', 'í‘œí˜„ ìŠ¤íƒ€ì¼', 'ìê¸° ì£¼ë„ì„±', 'ì™¸í–¥ì„±', 'í•´ê²° ì „ëµ', 'ì‘ë‹µ ê¸¸ì´', 'ë³µì¡ë„'],
                    f"ì§ˆë¬¸ {question_index + 1}: í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ íƒœê·¸ ë¹„êµ (ê¸°ë³¸ê°’)"
                )
        
        # íˆíŠ¸ë§µ ìƒì„±
        if analysis_type in ["heatmap", "all"]:
            try:
                print(f"ğŸ” íˆíŠ¸ë§µ ë””ë²„ê¹…: responses_data í‚¤ ê°œìˆ˜ = {len(responses_data)}")
                print(f"ğŸ” íˆíŠ¸ë§µ ë””ë²„ê¹…: ì²« ë²ˆì§¸ ì‘ë‹µ ë°ì´í„° í‚¤ = {list(responses_data.keys())[:3]}")
                
                heatmap_data = {}
                for persona, analysis in responses_data.items():
                    # ëª¨ë“  ì°¨ì›ì— ëŒ€í•œ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
                    all_dimensions = ['emotional_intensity', 'valence', 'expression_type', 
                                    'agency', 'extroversion', 'solution_offered', 'response_length', 'complexity']
                    values = []
                    for dim in all_dimensions:
                        values.append(analysis.get(dim, 0.0))  # ê¸°ë³¸ê°’ 0.0
                    heatmap_data[persona] = values
                    print(f"ğŸ” íˆíŠ¸ë§µ ë””ë²„ê¹…: {persona} = {values}")
                
                print(f"ğŸ” íˆíŠ¸ë§µ ë””ë²„ê¹…: heatmap_data í‚¤ ê°œìˆ˜ = {len(heatmap_data)}")
                print(f"ğŸ” íˆíŠ¸ë§µ ë””ë²„ê¹…: ì²« ë²ˆì§¸ ê°’ ê¸¸ì´ = {len(list(heatmap_data.values())[0])}")
                
                dimension_labels = all_dimensions
                
                # íˆíŠ¸ë§µ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ìƒì„± ë°©ë²• ì‚¬ìš©
                if request.heatmap_type == "plotly":
                    result['heatmap'] = viz_generator.generate_plotly_heatmap(
                        heatmap_data,
                        dimension_labels,
                        f"ì§ˆë¬¸ {question_index + 1}: í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ ë¶„ì„ (Plotly)"
                    )
                    result['heatmap_type'] = "plotly"
                else:
                    result['heatmap'] = viz_generator.generate_heatmap(
                        heatmap_data,
                        dimension_labels,
                        f"ì§ˆë¬¸ {question_index + 1}: í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ ë¶„ì„ (Seaborn)"
                    )
                    result['heatmap_type'] = "seaborn"
            except Exception as e:
                print(f"âŒ íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ íˆíŠ¸ë§µ ë°ì´í„° ìƒì„±
                fallback_heatmap = {}
                for persona in responses_data.keys():
                    fallback_heatmap[persona] = [1.0, 2.0, 3.0, 2.5, 2.0, 2.5, 3.0, 2.5]
                
                # fallback íˆíŠ¸ë§µë„ íƒ€ì…ì— ë”°ë¼ ìƒì„±
                if request.heatmap_type == "plotly":
                    result['heatmap'] = viz_generator.generate_plotly_heatmap(
                        fallback_heatmap,
                        ['ê°ì • ê°•ë„', 'ì •ì„œ ë°©í–¥', 'í‘œí˜„ ìŠ¤íƒ€ì¼', 'ìê¸° ì£¼ë„ì„±', 'ì™¸í–¥ì„±', 'í•´ê²° ì „ëµ', 'ì‘ë‹µ ê¸¸ì´', 'ë³µì¡ë„'],
                        f"ì§ˆë¬¸ {question_index + 1}: í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ ë¶„ì„ (ê¸°ë³¸ê°’ - Plotly)"
                    )
                    result['heatmap_type'] = "plotly"
                else:
                    result['heatmap'] = viz_generator.generate_heatmap(
                        fallback_heatmap,
                        ['ê°ì • ê°•ë„', 'ì •ì„œ ë°©í–¥', 'í‘œí˜„ ìŠ¤íƒ€ì¼', 'ìê¸° ì£¼ë„ì„±', 'ì™¸í–¥ì„±', 'í•´ê²° ì „ëµ', 'ì‘ë‹µ ê¸¸ì´', 'ë³µì¡ë„'],
                        f"ì§ˆë¬¸ {question_index + 1}: í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ ë¶„ì„ (ê¸°ë³¸ê°’ - Seaborn)"
                    )
                    result['heatmap_type'] = "seaborn"
        
        # ì •ë ¬ ì°¨íŠ¸ ìƒì„±
        if analysis_type in ["sorting", "all"]:
            result['sorting_chart'] = viz_generator.generate_sorting_chart(
                dimension_values,
                f"ì§ˆë¬¸ {question_index + 1}: {dimension} ê¸°ì¤€ ì •ë ¬"
            )
            
            # ì •ë ¬ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë„ ì¶”ê°€
            sorted_personas = sorted(dimension_values.items(), key=lambda x: x[1], reverse=True)
            result['sorting_list'] = {
                'dimension': dimension,
                'dimension_name': viz_generator.get_dimension_display_name(dimension),
                'sorted_data': [
                    {
                        'rank': i + 1,
                        'personality': persona,
                        'value': round(value, 2),
                        'display_name': f'í˜ë¥´ì†Œë‚˜ {persona}'
                    }
                    for i, (persona, value) in enumerate(sorted_personas)
                ]
            }
        
        return {
            "success": True,
            "data": result,
            "message": "ì‹œê°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"âŒ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {error_details}")
        raise HTTPException(status_code=500, detail=f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.post("/sample")
async def generate_sample_visualizations():
    """ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹œê°í™” ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
    try:
        # ìƒ˜í”Œ ë°ì´í„°
        sample_data = {
            'A': [4.2, 4.5, 3, 4.8, 3.2, 5.0, 3.5, 3.8],
            'B': [3.1, 2.8, 5, 2.5, 4.5, 3.0, 4.2, 3.1],
            'C': [5.0, 1.5, 1, 1.8, 2.1, 1.0, 2.8, 2.5]
        }
        
        labels = ['ê°ì • ê°•ë„', 'ì •ì„œ ë°©í–¥', 'í‘œí˜„ ìŠ¤íƒ€ì¼', 'ìê¸° ì£¼ë„ì„±', 'ì™¸í–¥ì„±', 'í•´ê²° ì „ëµ', 'ì‘ë‹µ ê¸¸ì´', 'ë³µì¡ë„']
        
        result = {
            'radar_chart': viz_generator.generate_radar_chart(
                sample_data, 
                labels,
                "ìƒ˜í”Œ: í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ íƒœê·¸ ë¹„êµ"
            ),
            'heatmap': viz_generator.generate_heatmap(
                sample_data,
                labels,
                "ìƒ˜í”Œ: í˜ë¥´ì†Œë‚˜ë³„ ì˜ë¯¸ ë¶„ì„ íˆíŠ¸ë§µ"
            ),
            'sorting_chart': viz_generator.generate_sorting_chart(
                {k: v[0] for k, v in sample_data.items()},  # ì²« ë²ˆì§¸ ì°¨ì›ìœ¼ë¡œ ì •ë ¬
                "ìƒ˜í”Œ: ê°ì • ê°•ë„ ê¸°ì¤€ ì •ë ¬"
            ),
            'sorting_list': {
                'dimension': 'agency',
                'dimension_name': 'ìê¸° ì£¼ë„ì„±',
                'sorted_data': [
                    {'rank': 1, 'personality': 'A', 'value': 4.8, 'display_name': 'í˜ë¥´ì†Œë‚˜ A'},
                    {'rank': 2, 'personality': 'B', 'value': 2.5, 'display_name': 'í˜ë¥´ì†Œë‚˜ B'},
                    {'rank': 3, 'personality': 'C', 'value': 1.8, 'display_name': 'í˜ë¥´ì†Œë‚˜ C'}
                ]
            }
        }
        
        return {
            "success": True,
            "data": result,
            "message": "ìƒ˜í”Œ ì‹œê°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìƒ˜í”Œ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.post("/diff")
async def generate_side_by_side_diff(request: DiffRequest):
    """ë‘ í˜ë¥´ì†Œë‚˜ì˜ ì‘ë‹µì„ Side-by-Sideë¡œ ë¹„êµ ë¶„ì„"""
    try:
        experiment_data = request.experiment_data
        question_index = request.question_index
        persona_a = request.persona_a
        persona_b = request.persona_b
        
        print(f"ğŸ” Side-by-Side Diff ë””ë²„ê¹…: persona_a = {persona_a}, persona_b = {persona_b}")
        print(f"ğŸ” Side-by-Side Diff ë””ë²„ê¹…: question_index = {question_index}")
        
        # ì‘ë‹µ ë°ì´í„° ì°¾ê¸°
        response_a = ""
        response_b = ""
        question_text = ""
        
        # ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡° ì‹œë„
        prompts_data = []
        
        print(f"ğŸ” ì‹¤í—˜ ë°ì´í„° êµ¬ì¡° ìƒì„¸ ë¶„ì„:")
        print(f"ğŸ” ìµœìƒìœ„ í‚¤ë“¤: {list(experiment_data.keys())}")
        
        if 'history' in experiment_data and isinstance(experiment_data['history'], list) and len(experiment_data['history']) > 0:
            history_item = experiment_data['history'][0]
            print(f"ğŸ” history[0] í‚¤ë“¤: {list(history_item.keys())}")
            print(f"ğŸ” history[0] íƒ€ì…: {type(history_item)}")
            
            # ë°©ë²• 1: history[0].prompts êµ¬ì¡° (ìš°ì„  ì²˜ë¦¬ - ì‹¤ì œ ë°ì´í„° êµ¬ì¡°)
            if 'prompts' in history_item:
                prompts_data = history_item['prompts']
                print(f"ğŸ” ë””ë²„ê¹…: history[0].promptsì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
                if prompts_data:
                    print(f"ğŸ” ì²« ë²ˆì§¸ prompt ìƒ˜í”Œ: {prompts_data[0] if len(prompts_data) > 0 else 'None'}")
            
            # ë°©ë²• 2: history[0].answers êµ¬ì¡° (ì´ì „ êµ¬ì¡° ì§€ì›)
            elif 'answers' in history_item:
                prompts_data = history_item['answers']
                print(f"ğŸ” ë””ë²„ê¹…: history[0].answersì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
                if prompts_data:
                    print(f"ğŸ” ì²« ë²ˆì§¸ answer ìƒ˜í”Œ: {prompts_data[0] if len(prompts_data) > 0 else 'None'}")
            
            # ë°©ë²• 3: history[0] ìì²´ê°€ ë°ì´í„° ë°°ì—´ì¸ ê²½ìš°
            elif isinstance(history_item, list):
                prompts_data = history_item
                print(f"ğŸ” ë””ë²„ê¹…: history[0]ì´ ë°°ì—´ì„, ê°œìˆ˜ = {len(prompts_data)}")
                if prompts_data:
                    print(f"ğŸ” ì²« ë²ˆì§¸ í•­ëª© ìƒ˜í”Œ: {prompts_data[0] if len(prompts_data) > 0 else 'None'}")
            
            # ë°©ë²• 4: history[0]ì— ë‹¤ë¥¸ í‚¤ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
            else:
                print(f"ğŸ” history[0]ì—ì„œ prompts/answersë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ë‹¤ë¥¸ í‚¤ë“¤ í™•ì¸:")
                for key, value in history_item.items():
                    if isinstance(value, list):
                        print(f"ğŸ” {key}: ë¦¬ìŠ¤íŠ¸ (ê¸¸ì´: {len(value)})")
                        if len(value) > 0:
                            print(f"ğŸ” {key}[0] ìƒ˜í”Œ: {value[0]}")
                    else:
                        print(f"ğŸ” {key}: {type(value)} = {value}")
        
        # ë°©ë²• 5: ì§ì ‘ prompts í‚¤
        elif 'prompts' in experiment_data:
            prompts_data = experiment_data['prompts']
            print(f"ğŸ” ë””ë²„ê¹…: ì§ì ‘ prompts í‚¤ì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
        
        # ë°©ë²• 6: ì§ì ‘ answers í‚¤
        elif 'answers' in experiment_data:
            prompts_data = experiment_data['answers']
            print(f"ğŸ” ë””ë²„ê¹…: ì§ì ‘ answers í‚¤ì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
        
        if prompts_data:
            # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì°¾ê¸°
            if 'questions' in experiment_data:
                questions = experiment_data['questions']
                if isinstance(questions, list) and len(questions) > question_index:
                    question_text = questions[question_index]
                elif isinstance(questions, dict) and 'prompts' in questions:
                    prompts = questions['prompts']
                    if isinstance(prompts, list) and len(prompts) > question_index:
                        question_text = prompts[question_index].get('text', f'ì§ˆë¬¸ {question_index + 1}')
            
            # ê° í˜ë¥´ì†Œë‚˜ì˜ ì‘ë‹µ ì°¾ê¸°
            print(f"ğŸ” í˜ë¥´ì†Œë‚˜ A ({persona_a})ì™€ B ({persona_b})ì˜ ì‘ë‹µì„ ì°¾ëŠ” ì¤‘...")
            
            for i, prompt in enumerate(prompts_data):
                # ë‹¤ì–‘í•œ í•„ë“œëª…ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ ì‹ë³„
                personality = prompt.get('personality', '') or prompt.get('persona', '') or prompt.get('name', '')
                print(f"ğŸ” [{i}] Prompt ë¶„ì„: personality='{personality}', keys={list(prompt.keys())}")
                
                if personality == persona_a:
                    print(f"ğŸ” í˜ë¥´ì†Œë‚˜ A ({persona_a}) ë°œê²¬! ì‘ë‹µ í•„ë“œ ì°¾ëŠ” ì¤‘...")
                    # ë°©ë²• 1: answer í•„ë“œ ì°¾ê¸°
                    if 'answer' in prompt and prompt['answer']:
                        response_a = prompt['answer']
                        print(f"âœ… í˜ë¥´ì†Œë‚˜ A ì‘ë‹µ ì°¾ìŒ (answer): ê¸¸ì´={len(response_a)}")
                    # ë°©ë²• 2: qa ë°°ì—´ì—ì„œ answer ì°¾ê¸°
                    elif 'qa' in prompt and isinstance(prompt['qa'], list) and len(prompt['qa']) > question_index:
                        qa_item = prompt['qa'][question_index]
                        if isinstance(qa_item, dict) and 'answer' in qa_item and qa_item['answer']:
                            response_a = qa_item['answer']
                            print(f"âœ… í˜ë¥´ì†Œë‚˜ A ì‘ë‹µ ì°¾ìŒ (qa[{question_index}].answer): ê¸¸ì´={len(response_a)}")
                    # ë°©ë²• 3: response í•„ë“œ ì°¾ê¸°
                    elif 'response' in prompt and prompt['response']:
                        response_a = prompt['response']
                        print(f"âœ… í˜ë¥´ì†Œë‚˜ A ì‘ë‹µ ì°¾ìŒ (response): ê¸¸ì´={len(response_a)}")
                    # ë°©ë²• 4: text í•„ë“œ ì°¾ê¸°
                    elif 'text' in prompt and prompt['text']:
                        response_a = prompt['text']
                        print(f"âœ… í˜ë¥´ì†Œë‚˜ A ì‘ë‹µ ì°¾ìŒ (text): ê¸¸ì´={len(response_a)}")
                    # ë°©ë²• 5: content í•„ë“œ ì°¾ê¸°
                    elif 'content' in prompt and prompt['content']:
                        response_a = prompt['content']
                        print(f"âœ… í˜ë¥´ì†Œë‚˜ A ì‘ë‹µ ì°¾ìŒ (content): ê¸¸ì´={len(response_a)}")
                    else:
                        print(f"âŒ í˜ë¥´ì†Œë‚˜ A ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ: {list(prompt.keys())}")
                
                elif personality == persona_b:
                    print(f"ğŸ” í˜ë¥´ì†Œë‚˜ B ({persona_b}) ë°œê²¬! ì‘ë‹µ í•„ë“œ ì°¾ëŠ” ì¤‘...")
                    # ë°©ë²• 1: answer í•„ë“œ ì°¾ê¸°
                    if 'answer' in prompt and prompt['answer']:
                        response_b = prompt['answer']
                        print(f"âœ… í˜ë¥´ì†Œë‚˜ B ì‘ë‹µ ì°¾ìŒ (answer): ê¸¸ì´={len(response_b)}")
                    # ë°©ë²• 2: qa ë°°ì—´ì—ì„œ answer ì°¾ê¸°
                    elif 'qa' in prompt and isinstance(prompt['qa'], list) and len(prompt['qa']) > question_index:
                        qa_item = prompt['qa'][question_index]
                        if isinstance(qa_item, dict) and 'answer' in qa_item and qa_item['answer']:
                            response_b = qa_item['answer']
                            print(f"âœ… í˜ë¥´ì†Œë‚˜ B ì‘ë‹µ ì°¾ìŒ (qa[{question_index}].answer): ê¸¸ì´={len(response_b)}")
                    # ë°©ë²• 3: response í•„ë“œ ì°¾ê¸°
                    elif 'response' in prompt and prompt['response']:
                        response_b = prompt['response']
                        print(f"âœ… í˜ë¥´ì†Œë‚˜ B ì‘ë‹µ ì°¾ìŒ (response): ê¸¸ì´={len(response_b)}")
                    # ë°©ë²• 4: text í•„ë“œ ì°¾ê¸°
                    elif 'text' in prompt and prompt['text']:
                        response_b = prompt['text']
                        print(f"âœ… í˜ë¥´ì†Œë‚˜ B ì‘ë‹µ ì°¾ìŒ (text): ê¸¸ì´={len(response_b)}")
                    # ë°©ë²• 5: content í•„ë“œ ì°¾ê¸°
                    elif 'content' in prompt and prompt['content']:
                        response_b = prompt['content']
                        print(f"âœ… í˜ë¥´ì†Œë‚˜ B ì‘ë‹µ ì°¾ìŒ (content): ê¸¸ì´={len(response_b)}")
                    else:
                        print(f"âŒ í˜ë¥´ì†Œë‚˜ B ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ: {list(prompt.keys())}")
                
                # ë‘˜ ë‹¤ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                if response_a and response_b:
                    print(f"ğŸ‰ ë‘ í˜ë¥´ì†Œë‚˜ì˜ ì‘ë‹µì„ ëª¨ë‘ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    break
        
        print(f"ğŸ” Side-by-Side Diff ë””ë²„ê¹…: response_a ê¸¸ì´ = {len(response_a)}")
        print(f"ğŸ” Side-by-Side Diff ë””ë²„ê¹…: response_b ê¸¸ì´ = {len(response_b)}")
        
        # ì‘ë‹µì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ëŒ€ì²´ ë°©ë²• ì‹œë„
        if not response_a or not response_b:
            print(f"âš ï¸ ê¸°ë³¸ ë°©ë²•ìœ¼ë¡œ ì‘ë‹µì„ ì°¾ì§€ ëª»í•¨. ëŒ€ì²´ ë°©ë²• ì‹œë„...")
            print(f"ğŸ” response_a ê¸¸ì´: {len(response_a)}, response_b ê¸¸ì´: {len(response_b)}")
            
            # ëŒ€ì²´ ë°©ë²•: ëª¨ë“  ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©° ì‘ë‹µ ì°¾ê¸°
            if not response_a or not response_b:
                print(f"ğŸ” ëŒ€ì²´ ë°©ë²•: ì „ì²´ ë°ì´í„° êµ¬ì¡°ì—ì„œ ì‘ë‹µ ì°¾ê¸° ì‹œë„...")
                
                # ì‹¤í—˜ ë°ì´í„°ì˜ ëª¨ë“  í‚¤ë¥¼ í™•ì¸
                all_keys = list(experiment_data.keys())
                print(f"ğŸ” ì‹¤í—˜ ë°ì´í„° ìµœìƒìœ„ í‚¤ë“¤: {all_keys}")
                
                # history ì™¸ì˜ ë‹¤ë¥¸ í‚¤ë“¤ë„ í™•ì¸
                for key in all_keys:
                    if key != 'history' and isinstance(experiment_data[key], list):
                        print(f"ğŸ” {key} í‚¤ì—ì„œ ë°ì´í„° í™•ì¸ ì¤‘... (ê¸¸ì´: {len(experiment_data[key])})")
                        
                        for item in experiment_data[key]:
                            if isinstance(item, dict):
                                # í˜ë¥´ì†Œë‚˜ í•„ë“œ í™•ì¸
                                item_personality = item.get('personality', '') or item.get('personality', '') or item.get('name', '')
                                
                                if item_personality == persona_a and not response_a:
                                    # ì‘ë‹µ í•„ë“œ ì°¾ê¸°
                                    for resp_field in ['answer', 'response', 'text', 'content', 'message']:
                                        if resp_field in item and item[resp_field]:
                                            response_a = item[resp_field]
                                            print(f"âœ… ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ A ì‘ë‹µ ì°¾ìŒ ({resp_field}): ê¸¸ì´={len(response_a)}")
                                            break
                                
                                elif item_personality == persona_b and not response_b:
                                    # ì‘ë‹µ í•„ë“œ ì°¾ê¸°
                                    for resp_field in ['answer', 'response', 'text', 'content', 'message']:
                                        if resp_field in item and item[resp_field]:
                                            response_b = item[resp_field]
                                            print(f"âœ… ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ B ì‘ë‹µ ì°¾ìŒ ({resp_field}): ê¸¸ì´={len(response_b)}")
                                            break
                                
                                if response_a and response_b:
                                    break
                        
                        if response_a and response_b:
                            break
            
            # ì—¬ì „íˆ ì°¾ì§€ ëª»í•œ ê²½ìš°
            if not response_a or not response_b:
                print(f"âŒ ëª¨ë“  ë°©ë²•ì„ ì‹œë„í–ˆì§€ë§Œ ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                print(f"ğŸ” ìµœì¢… ìƒíƒœ: response_a ê¸¸ì´={len(response_a)}, response_b ê¸¸ì´={len(response_b)}")
                raise HTTPException(status_code=400, detail="ë‘ í˜ë¥´ì†Œë‚˜ì˜ ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if not question_text:
            question_text = f"ì§ˆë¬¸ {question_index + 1}"
        
        # Side-by-Side Diff ìƒì„±
        diff_result = viz_generator.generate_side_by_side_diff(
            response_a, response_b, persona_a, persona_b, question_text
        )
        
        return {
            "success": True,
            "data": diff_result,
            "message": "Side-by-Side Diffê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"âŒ Side-by-Side Diff ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {error_details}")
        raise HTTPException(status_code=500, detail=f"Side-by-Side Diff ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.post("/structured-analysis")
async def generate_structured_analysis(request: StructuredAnalysisRequest):
    """êµ¬ì¡°í™”ëœ JSON í˜•íƒœë¡œ ì˜ë¯¸ íƒœê¹… ë° ë¶„ì„ ê²°ê³¼ ìƒì„±"""
    try:
        experiment_data = request.experiment_data
        question_index = request.question_index
        include_embeddings = request.include_embeddings
        include_similarities = request.include_similarities
        analysis_depth = request.analysis_depth
        
        # ì‹¤í—˜ëª… ì¶”ì¶œ
        experiment_name = experiment_data.get('experiment_name', f'experiment_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
        
        print(f"ğŸ”„ ìƒˆë¡œìš´ êµ¬ì¡°í™”ëœ ë¶„ì„ ìƒì„± ì‹œì‘: {experiment_name}, Q{question_index}")
        
        print(f"ğŸ” êµ¬ì¡°í™”ëœ ë¶„ì„ ì‹œì‘: question_index = {question_index}")
        print(f"ğŸ” ë¶„ì„ ì˜µì…˜: embeddings={include_embeddings}, similarities={include_similarities}, depth={analysis_depth}")
        
        # ì‘ë‹µ ë°ì´í„° ìˆ˜ì§‘
        responses_data = {}
        question_text = ""
        
        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ ì§ˆë¬¸ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if request.question_text:
            question_text = request.question_text
            print(f"âœ… í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {question_text[:100]}...")
        
        # ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡° ì‹œë„
        prompts_data = []
        
        print(f"ğŸ” êµ¬ì¡°í™”ëœ ë¶„ì„ ë””ë²„ê¹…: experiment_data í‚¤ë“¤ = {list(experiment_data.keys())}")
        
        # ë°©ë²• 1: history[0].answers êµ¬ì¡° (ì‹¤ì œ ë°ì´í„° êµ¬ì¡° - ìš°ì„  ì²˜ë¦¬)
        if 'history' in experiment_data and isinstance(experiment_data['history'], list) and len(experiment_data['history']) > 0:
            print(f"ğŸ” êµ¬ì¡°í™”ëœ ë¶„ì„ ë””ë²„ê¹…: history[0] í‚¤ë“¤ = {list(experiment_data['history'][0].keys())}")
            if 'answers' in experiment_data['history'][0]:
                prompts_data = experiment_data['history'][0]['answers']
                print(f"ğŸ” ë””ë²„ê¹…: history[0].answersì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
                if prompts_data and len(prompts_data) > 0:
                    print(f"ğŸ” êµ¬ì¡°í™”ëœ ë¶„ì„ ë””ë²„ê¹…: ì²« ë²ˆì§¸ answer í‚¤ë“¤ = {list(prompts_data[0].keys())}")
        
        # ë°©ë²• 2: history[0].prompts êµ¬ì¡° (ëŒ€ì•ˆ êµ¬ì¡°)
        elif 'history' in experiment_data and isinstance(experiment_data['history'], list) and len(experiment_data['history']) > 0:
            if 'prompts' in experiment_data['history'][0]:
                prompts_data = experiment_data['history'][0]['prompts']
                print(f"ğŸ” ë””ë²„ê¹…: history[0].promptsì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
                if prompts_data and len(prompts_data) > 0:
                    print(f"ğŸ” êµ¬ì¡°í™”ëœ ë¶„ì„ ë””ë²„ê¹…: ì²« ë²ˆì§¸ prompt í‚¤ë“¤ = {list(prompts_data[0].keys())}")
        
        # ë°©ë²• 3: ì§ì ‘ prompts í‚¤
        elif 'prompts' in experiment_data:
            prompts_data = experiment_data['prompts']
            print(f"ğŸ” ë””ë²„ê¹…: ì§ì ‘ prompts í‚¤ì—ì„œ ë°ì´í„° ì°¾ìŒ, ê°œìˆ˜ = {len(prompts_data)}")
            if prompts_data and len(prompts_data) > 0:
                print(f"ğŸ” êµ¬ì¡°í™”ëœ ë¶„ì„ ë””ë²„ê¹…: ì²« ë²ˆì§¸ prompt í‚¤ë“¤ = {list(prompts_data[0].keys())}")
        
        if not prompts_data:
            print(f"âŒ êµ¬ì¡°í™”ëœ ë¶„ì„ ë””ë²„ê¹…: prompts_dataë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print(f"âŒ êµ¬ì¡°í™”ëœ ë¶„ì„ ë””ë²„ê¹…: experiment_data êµ¬ì¡° = {experiment_data}")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ í™”ë©´ì— ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ëœ¨ì§€ ì•Šë„ë¡ í•¨
            return {
                "success": True,
                "data": {
                    "question_id": f"Q{question_index + 1}",
                    "question_text": f"ì§ˆë¬¸ {question_index + 1}",
                    "persona_responses": [],
                    "similarities": [],
                    "overall_statistics": {},
                    "model_info": {
                        "llm_model": "N/A",
                        "embedding_model": "N/A",
                        "analysis_version": "1.0.0"
                    }
                },
                "message": "ë¶„ì„í•  ì‘ë‹µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì°¾ê¸° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©)
        if not question_text:  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ë•Œë§Œ ê²€ìƒ‰
            print(f"ğŸ” ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘: question_index={question_index}")
            
            # ë°©ë²• 1: questions ë°°ì—´ì—ì„œ ì§ì ‘ ê²€ìƒ‰
            if 'questions' in experiment_data:
                questions = experiment_data['questions']
                print(f"ğŸ” questions í‚¤ ë°œê²¬: {type(questions)}")
                
                if isinstance(questions, list) and len(questions) > question_index:
                    question_text = questions[question_index]
                    print(f"âœ… questions ë°°ì—´ì—ì„œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì°¾ìŒ: {question_text[:100]}...")
                elif isinstance(questions, dict):
                    print(f"ğŸ” questions ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: {list(questions.keys())}")
                    if 'prompts' in questions:
                        prompts = questions['prompts']
                        if isinstance(prompts, list) and len(prompts) > question_index:
                            question_text = prompts[question_index].get('text', '')
                            if question_text:
                                print(f"âœ… questions.promptsì—ì„œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì°¾ìŒ: {question_text[:100]}...")
            
            # ë°©ë²• 2: history[0].questionsì—ì„œ ê²€ìƒ‰
            if not question_text and 'history' in experiment_data and isinstance(experiment_data['history'], list) and len(experiment_data['history']) > 0:
                history_questions = experiment_data['history'][0].get('questions', [])
                if isinstance(history_questions, list) and len(history_questions) > question_index:
                    question_text = history_questions[question_index]
                    print(f"âœ… history[0].questionsì—ì„œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì°¾ìŒ: {question_text[:100]}...")
            
            # ë°©ë²• 3: prompts_dataì—ì„œ ì§ˆë¬¸ ì°¾ê¸°
            if not question_text and prompts_data:
                print(f"ğŸ” prompts_dataì—ì„œ ì§ˆë¬¸ ê²€ìƒ‰ ì‹œë„...")
                for prompt in prompts_data:
                    if 'question' in prompt and prompt['question']:
                        question_text = prompt['question']
                        print(f"âœ… prompts_dataì—ì„œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì°¾ìŒ: {question_text[:100]}...")
                        break
                    elif 'qa' in prompt and isinstance(prompt['qa'], list) and len(prompt['qa']) > question_index:
                        qa_item = prompt['qa'][question_index]
                        if isinstance(qa_item, dict) and 'question' in qa_item and qa_item['question']:
                            question_text = prompt['question']
                            print(f"âœ… prompts_data.qaì—ì„œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì°¾ìŒ: {question_text[:100]}...")
                            break
            
            # ë°©ë²• 4: ê¸°ë³¸ê°’ ì„¤ì •
            if not question_text:
                question_text = f"ì§ˆë¬¸ {question_index + 1}"
                print(f"âš ï¸ ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©: {question_text}")
            else:
                print(f"âœ… ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ ì§ˆë¬¸ í…ìŠ¤íŠ¸: {question_text[:100]}...")
        else:
            print(f"âœ… í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {question_text[:100]}...")
        
        # ê° í˜ë¥´ì†Œë‚˜ì˜ ì‘ë‹µ ë¶„ì„
        persona_responses = []
        for prompt in prompts_data:
            personality = prompt.get('personality', 'Unknown')
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì°¾ê¸° (ì•ˆì „í•œ ì²˜ë¦¬)
            response_text = ""
            try:
                if 'answer' in prompt and prompt['answer']:
                    response_text = str(prompt['answer'])
                elif 'qa' in prompt and isinstance(prompt['qa'], list) and len(prompt['qa']) > question_index:
                    qa_item = prompt['qa'][question_index]
                    if isinstance(qa_item, dict) and 'answer' in qa_item and qa_item['answer']:
                        response_text = str(qa_item['answer'])
                elif 'response' in prompt and prompt['response']:
                    response_text = str(prompt['response'])
                elif 'text' in prompt and prompt['text']:
                    response_text = str(prompt['text'])
                
                print(f"ğŸ” ì‘ë‹µ í…ìŠ¤íŠ¸ ì°¾ê¸°: personality={personality}, í‚¤={list(prompt.keys())}, response_text ê¸¸ì´={len(response_text) if response_text else 0}")
            except Exception as e:
                print(f"âŒ ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}, personality={personality}")
                continue
            
            if not response_text or not response_text.strip():
                print(f"âš ï¸ ë¹ˆ ì‘ë‹µ í…ìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°: personality={personality}")
                continue
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
            if not isinstance(response_text, str):
                print(f"âš ï¸ ì‘ë‹µ í…ìŠ¤íŠ¸ê°€ ë¬¸ìì—´ì´ ì•„ë‹˜: {type(response_text)}, personality={personality}")
                continue
            
            print(f"ğŸ” ì‘ë‹µ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: personality={personality}, ê¸¸ì´={len(response_text)}")
            
            # LLM íƒœê¹… ìˆ˜í–‰
            llm_tags = viz_generator.llm_tagger.tag_response(response_text)
            
            # ì˜ë¯¸ ì¶• ì ìˆ˜ëŠ” ì œê±° (ê°„ë‹¨í•œ êµ¬ì¡°í™”ëœ ë¶„ì„)
            
            # ì„ë² ë”© ìƒì„± (ìš”ì²­ëœ ê²½ìš°)
            embedding = None
            if include_embeddings and sentence_model:
                try:
                    embedding = sentence_model.encode([response_text])[0].tolist()
                except Exception as e:
                    print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # ë©”íƒ€ë°ì´í„°ëŠ” ì œê±° (ê°„ë‹¨í•œ êµ¬ì¡°í™”ëœ ë¶„ì„)
            
            # PersonaTag ê°ì²´ ìƒì„±
            tags = PersonaTag(
                ê°ì •_ë°©í–¥=llm_tags.get("ê°ì • ë°©í–¥", "ì¤‘ë¦½"),
                ê°ì •_ê°•ë„=llm_tags.get("ê°ì • ê°•ë„", "ë³´í†µ"),
                í–‰ë™_ì„±í–¥=llm_tags.get("í–‰ë™ ì„±í–¥", "ìˆ˜ë™ì "),
                ê´€ê³„_ì§€í–¥ì„±=llm_tags.get("ê´€ê³„ ì§€í–¥ì„±", "ìê¸°ì¤‘ì‹¬"),
                ì§€ì›_ìš”ì²­_ì—¬ë¶€=llm_tags.get("ì§€ì› ìš”ì²­ ì—¬ë¶€", "ì—†ìŒ"),
                í‘œí˜„_ìŠ¤íƒ€ì¼=_determine_expression_style(response_text)
            )
            
            # ì˜ë¯¸ ì¶• ì ìˆ˜ ê³„ì‚°
            analysis_scores = viz_generator.analyze_response_dimensions(response_text)
            
            # PersonaResponse ê°ì²´ ìƒì„±
            persona_response = PersonaResponse(
                persona=personality,
                text=response_text,
                embedding=embedding,
                tags=tags,
                analysis_scores=analysis_scores,
                metadata={}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •
            )
            
            persona_responses.append(persona_response)
            responses_data[personality] = analysis_scores
        
        if not persona_responses:
            if not prompts_data:
                print("âŒ ë¶„ì„ ê°€ëŠ¥í•œ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤: answers ë°°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            else:
                print("âŒ ë¶„ì„ ê°€ëŠ¥í•œ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤: ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ í™”ë©´ì— ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ëœ¨ì§€ ì•Šë„ë¡ í•¨
            return {
                "success": True,
                "data": {
                    "question_id": f"Q{question_index + 1}",
                    "question_text": question_text,
                    "persona_responses": [],
                    "similarities": [],
                    "overall_statistics": {},
                    "model_info": {
                        "llm_model": "N/A",
                        "embedding_model": "N/A",
                        "analysis_version": "1.0.0"
                    }
                },
                "message": "ë¶„ì„ ê°€ëŠ¥í•œ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # í˜ë¥´ì†Œë‚˜ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ìš”ì²­ëœ ê²½ìš°)
        similarities = []
        if include_similarities and len(persona_responses) > 1:
            for i in range(len(persona_responses)):
                for j in range(i + 1, len(persona_responses)):
                    persona_a = persona_responses[i].persona
                    persona_b = persona_responses[j].persona
                    text_a = persona_responses[i].text
                    text_b = persona_responses[j].text
                    
                    similarity_score = viz_generator._calculate_similarity(text_a, text_b)
                    
                    similarity = SimilarityScore(
                        persona_a=persona_a,
                        persona_b=persona_b,
                        score=similarity_score,
                        similarity_type="cosine"
                    )
                    similarities.append(similarity)
        
        # ëª¨ë¸ ì •ë³´
        model_info = {
            "llm_model": viz_generator.llm_tagger.model,
            "embedding_model": "sentence-transformers/all-MiniLM-L6-v2" if sentence_model else "N/A",
            "analysis_version": "1.0.0"
        }
        
        # AnalysisResult ê°ì²´ ìƒì„±
        analysis_result = AnalysisResult(
            question_id=f"Q{question_index + 1}",
            question_text=question_text,
            persona_responses=persona_responses,
            similarities=similarities,
            overall_statistics={},  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •
            model_info=model_info
        )
        
        return {
            "success": True,
            "data": analysis_result.dict(),
            "message": "êµ¬ì¡°í™”ëœ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"âŒ êµ¬ì¡°í™”ëœ ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {error_details}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ í™”ë©´ì— ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ëœ¨ì§€ ì•Šë„ë¡ í•¨
        return {
            "success": True,
            "data": {
                "question_id": f"Q{question_index + 1}",
                "question_text": f"ì§ˆë¬¸ {question_index + 1}",
                "persona_responses": [],
                "similarities": [],
                "overall_statistics": {},
                "model_info": {
                    "llm_model": "N/A",
                    "embedding_model": "N/A",
                    "analysis_version": "1.0.0"
                }
            },
            "message": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        }

def _determine_expression_style(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ í‘œí˜„ ìŠ¤íƒ€ì¼ì„ ê²°ì •"""
    comfort_words = ['ê´œì°®ì•„', 'í˜ë‚´', 'ì˜ë  ê±°ì•¼', 'ê±±ì •ë§ˆ', 'ìœ„ë¡œ', 'ì•ˆì‹¬', 'í¬ë§', 'ê¸°ëŒ€']
    info_words = ['ì •ë³´', 'ì‚¬ì‹¤', 'ë°ì´í„°', 'í†µê³„', 'ì—°êµ¬', 'ë¶„ì„', 'ê²°ê³¼', 'ì¦ê±°']
    command_words = ['í•´ì•¼', 'í•˜ì§€ë§ˆ', 'í•„ìš”í•´', 'ì¤‘ìš”í•´', 'ë‹¹ì—°í•´', 'ë¬´ì¡°ê±´']
    
    comfort_score = sum(text.count(word) for word in comfort_words) * 0.6
    info_score = sum(text.count(word) for word in info_words) * 0.7
    command_score = sum(text.count(word) for word in command_words) * 0.5
    
    style_scores = [comfort_score, info_score, command_score]
    max_style = max(style_scores)
    
    if max_style == comfort_score:
        return "ìœ„ë¡œí˜•"
    elif max_style == info_score:
        return "ì •ë³´í˜•"
    else:
        return "ëª…ë ¹í˜•"

# ì „ì²´ í†µê³„ í•¨ìˆ˜ ì œê±°ë¨

# ì €ì¥ ê¸°ëŠ¥ì€ ì œê±°ë¨

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (import í›„ì— ìƒì„±)
viz_generator = VisualizationGenerator()
