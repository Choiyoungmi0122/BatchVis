# Batch ì²˜ë¦¬ ì„¤ì • íŒŒì¼
# LLM íƒœê¹… ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •

# OpenAI API ì„¤ì •
OPENAI_CONFIG = {
    "model": "gpt-3.5-turbo",  # ì‚¬ìš©í•  ëª¨ë¸
    "temperature": 0.1,         # ì‘ë‹µ ì¼ê´€ì„±ì„ ìœ„í•œ ë‚®ì€ temperature
    "max_tokens": 1000,         # Batch ì²˜ë¦¬ìš© í† í° ìˆ˜ ì¦ê°€
    "timeout": 30,              # API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
}

# Batch ì²˜ë¦¬ ì„¤ì • (ê³ ì† ìµœì í™”)
BATCH_CONFIG = {
    "default_batch_size": 25,           # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸° (ì¦ê°€)
    "max_batch_size": 50,               # ìµœëŒ€ ë°°ì¹˜ í¬ê¸° (ì¦ê°€)
    "min_batch_size": 10,               # ìµœì†Œ ë°°ì¹˜ í¬ê¸° (ì¦ê°€)
    "api_call_delay": 0.05,             # API í˜¸ì¶œ ê°„ê²© (ê°ì†Œ)
    "max_retries": 2,                   # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê°ì†Œ)
    "retry_delay": 0.5,                 # ì¬ì‹œë„ ê°„ê²© (ê°ì†Œ)
}

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •
MONITORING_CONFIG = {
    "enable_logging": True,             # ë¡œê¹… í™œì„±í™”
    "log_batch_progress": True,         # ë°°ì¹˜ ì§„í–‰ë¥  ë¡œê¹…
    "log_performance_metrics": True,    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…
    "performance_threshold": 5.0,       # ì„±ëŠ¥ í–¥ìƒ ì„ê³„ê°’ (ë°°)
}

# Fallback ì„¤ì •
FALLBACK_CONFIG = {
    "enable_rule_based_fallback": True, # ê·œì¹™ ê¸°ë°˜ íƒœê¹… ëŒ€ì²´ í™œì„±í™”
    "enable_individual_fallback": True, # ê°œë³„ ì²˜ë¦¬ ëŒ€ì²´ í™œì„±í™”
    "fallback_priority": ["llm", "rule_based", "individual"], # ëŒ€ì²´ ìš°ì„ ìˆœìœ„
}

# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (ê³ ì† ìµœì í™”)
MEMORY_CONFIG = {
    "max_concurrent_batches": 5,        # ë™ì‹œ ì²˜ë¦¬í•  ìµœëŒ€ ë°°ì¹˜ ìˆ˜ (ì¦ê°€)
    "memory_cleanup_interval": 3,       # ë©”ëª¨ë¦¬ ì •ë¦¬ ê°„ê²© (ë°°ì¹˜ ìˆ˜) (ê°ì†Œ)
    "enable_garbage_collection": True,  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ í™œì„±í™”
}

# ì—ëŸ¬ ì²˜ë¦¬ ì„¤ì •
ERROR_HANDLING_CONFIG = {
    "continue_on_partial_failure": True, # ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰
    "log_error_details": True,           # ì—ëŸ¬ ìƒì„¸ ì •ë³´ ë¡œê¹…
    "max_consecutive_failures": 3,       # ì—°ì† ì‹¤íŒ¨ ìµœëŒ€ íšŸìˆ˜
}

# ì„±ëŠ¥ ìµœì í™” íŒ
PERFORMANCE_TIPS = {
    "optimal_batch_size": "ì‘ë‹µ ìˆ˜ê°€ 10ê°œ ì´ìƒì¼ ë•Œ ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™”",
    "api_rate_limit": "OpenAI API Rate Limitì„ ê³ ë ¤í•œ ì ì ˆí•œ ê°„ê²© ì„¤ì •",
    "memory_management": "ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§",
    "fallback_strategy": "LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ìë™ ì „í™˜",
}

def get_optimal_batch_size(response_count: int) -> int:
    """ì‘ë‹µ ìˆ˜ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
    if response_count <= BATCH_CONFIG["min_batch_size"]:
        return response_count
    elif response_count <= BATCH_CONFIG["default_batch_size"]:
        return BATCH_CONFIG["default_batch_size"]
    elif response_count <= BATCH_CONFIG["max_batch_size"]:
        return min(response_count, BATCH_CONFIG["max_batch_size"])
    else:
        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° ë°°ì¹˜ í¬ê¸° ì¡°ì •
        return min(BATCH_CONFIG["max_batch_size"], response_count // 2)

def should_use_batch_processing(response_count: int) -> bool:
    """ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
    return response_count >= BATCH_CONFIG["min_batch_size"]

def calculate_expected_processing_time(response_count: int, use_batch: bool = True) -> float:
    """ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ì´ˆ)"""
    if use_batch and should_use_batch_processing(response_count):
        batch_size = get_optimal_batch_size(response_count)
        num_batches = (response_count + batch_size - 1) // batch_size
        # ë°°ì¹˜ë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
        avg_batch_time = 8.0  # 10ê°œ ì‘ë‹µ ê¸°ì¤€
        return num_batches * avg_batch_time
    else:
        # ê°œë³„ ì²˜ë¦¬ ì‹œê°„
        return response_count * 3.0  # ì‘ë‹µë‹¹ í‰ê·  3ì´ˆ

def get_performance_improvement(response_count: int) -> float:
    """ì„±ëŠ¥ í–¥ìƒ ë°°ìˆ˜ ê³„ì‚°"""
    individual_time = calculate_expected_processing_time(response_count, use_batch=False)
    batch_time = calculate_expected_processing_time(response_count, use_batch=True)
    
    if batch_time > 0:
        return individual_time / batch_time
    return 1.0

# ì„¤ì • ê²€ì¦
def validate_config() -> bool:
    """ì„¤ì • ìœ íš¨ì„± ê²€ì¦"""
    try:
        assert BATCH_CONFIG["min_batch_size"] <= BATCH_CONFIG["default_batch_size"] <= BATCH_CONFIG["max_batch_size"]
        assert BATCH_CONFIG["api_call_delay"] >= 0
        assert BATCH_CONFIG["max_retries"] >= 0
        assert OPENAI_CONFIG["temperature"] >= 0 and OPENAI_CONFIG["temperature"] <= 2
        assert OPENAI_CONFIG["max_tokens"] > 0
        return True
    except AssertionError:
        print("âŒ ì„¤ì • ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨")
        return False

if __name__ == "__main__":
    # ì„¤ì • í…ŒìŠ¤íŠ¸
    print("ğŸ”§ Batch ì²˜ë¦¬ ì„¤ì • í…ŒìŠ¤íŠ¸")
    print(f"âœ… ì„¤ì • ìœ íš¨ì„±: {validate_config()}")
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    test_counts = [5, 10, 20, 50, 100]
    print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("ì‘ë‹µ ìˆ˜ | ê°œë³„ ì²˜ë¦¬ | ë°°ì¹˜ ì²˜ë¦¬ | ì„±ëŠ¥ í–¥ìƒ")
    print("-" * 50)
    
    for count in test_counts:
        individual = calculate_expected_processing_time(count, False)
        batch = calculate_expected_processing_time(count, True)
        improvement = get_performance_improvement(count)
        
        print(f"{count:6d} | {individual:8.1f}ì´ˆ | {batch:8.1f}ì´ˆ | {improvement:6.1f}ë°°")
    
    print(f"\nğŸš€ ìµœì  ë°°ì¹˜ í¬ê¸°: {BATCH_CONFIG['default_batch_size']}")
    print(f"ğŸ“ˆ ìµœëŒ€ ì„±ëŠ¥ í–¥ìƒ: {max(get_performance_improvement(c) for c in test_counts):.1f}ë°°")
